{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import hashlib\n",
    "import textwrap\n",
    "import traceback\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from unidecode import unidecode  # ggf. von LLM-Code genutzt\n",
    "\n",
    "# Pfade robust (Notebook oder Skript)\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "# Verzeichnisse\n",
    "CLEAN_DIR    = BASE_DIR / \"cleaned\"\n",
    "RAW_DIR      = BASE_DIR / \"data\" / \"raw\"\n",
    "ARTIFACT_DIR = BASE_DIR / \"run_artifacts\"\n",
    "INVALID_DIR  = BASE_DIR / \"data\" / \"invalide\"\n",
    "for d in (CLEAN_DIR, ARTIFACT_DIR, INVALID_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Konfiguration\n",
    "DATA_PATH  = RAW_DIR / \"rotten_tomatoes_movies.csv\"  # bei Bedarf auf andere Quelle setzen\n",
    "MODEL_NAME = \"o4-mini\"\n",
    "SAMPLE_SIZE = 5\n",
    "RNG_STATE   = 42\n",
    "\n",
    "# ENV\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Laufkontext\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Schema\n",
    "SCHEMA_PATH = BASE_DIR / \"schema.json\"\n",
    "SCHEMA_SPEC = json.loads(SCHEMA_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Helper\n",
    "def schema_as_text(spec: list[dict]) -> str:\n",
    "    return \"\\n\".join(f\"- {d['field']}: (type: {d['type']}) {d['rule']}\" for d in spec)\n",
    "\n",
    "def infer_dataset_tag(path: str | Path) -> str:\n",
    "    name = Path(path).name.lower()\n",
    "    if \"imdb\" in name: return \"IMDB\"\n",
    "    if \"movielens\" in name: return \"MOVIELENS\"\n",
    "    if \"metacritic\" in name: return \"METACRITIC\"\n",
    "    if \"rotten\" in name or \"rt\" in name: return \"RT\"\n",
    "    return \"DATA\"\n",
    "\n",
    "DATASET_TAG = infer_dataset_tag(DATA_PATH)\n",
    "ID_COL = f\"ID_{DATASET_TAG}\"\n",
    "SCHEMA_TEXT = schema_as_text(SCHEMA_SPEC)\n",
    "\n",
    "def load_df(path=DATA_PATH):\n",
    "    return pd.read_csv(path, on_bad_lines=\"skip\")\n",
    "\n",
    "def make_context_and_meta(df):\n",
    "    ctx = df.sample(SAMPLE_SIZE, random_state=RNG_STATE).to_string(index=False)\n",
    "    buf = StringIO(); df.info(buf=buf)\n",
    "    return ctx, buf.getvalue()\n",
    "\n",
    "def extract_code_block(md: str) -> str:\n",
    "    if \"```\" in md:\n",
    "        md = md.split(\"```\")[-2]\n",
    "    return md.replace(\"python\", \"\", 1).replace(\"```\", \"\").strip()\n",
    "\n",
    "def exec_generated_code(code: str, g: dict):\n",
    "    block = extract_code_block(code) if \"```\" in code else code\n",
    "    exec(block, g)\n",
    "\n",
    "# Artefakte & Logging\n",
    "def run_dir_for(dataset_path: str | Path) -> Path:\n",
    "    stem = Path(dataset_path).stem\n",
    "    d = ARTIFACT_DIR / f\"{stem}_{RUN_ID}\"\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def append_log(msg: str, dataset_path: str | Path, fname: str = \"run.log\"):\n",
    "    rd = run_dir_for(dataset_path)\n",
    "    with open(rd / fname, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg.rstrip() + \"\\n\")\n",
    "\n",
    "def save_artifact(text: str, stage: str, kind: str, dataset_path: str | Path, attempt: int = 0):\n",
    "    stem = Path(dataset_path).stem\n",
    "    suffix = \"_retry\" if attempt else \"\"\n",
    "    ext = \"txt\" if kind == \"prompt\" else \"py\"\n",
    "    target_dir = ARTIFACT_DIR / stem\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (target_dir / f\"{stage}_{kind}{suffix}.{ext}\").write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "def save_artifact_copy(text: str, stage: str, kind: str, dataset_path: str | Path,\n",
    "                       attempt: int = 0, add_hash: bool = False) -> str:\n",
    "    run_dir = run_dir_for(dataset_path)\n",
    "    suffix = \"_retry\" if attempt else \"\"\n",
    "    ext = \"txt\" if kind == \"prompt\" else \"py\"\n",
    "    fname = f\"{stage}_{kind}{suffix}\"\n",
    "    if add_hash:\n",
    "        digest = hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:12]\n",
    "        fname += f\"_{digest}\"\n",
    "    target = run_dir / f\"{fname}.{ext}\"\n",
    "    target.write_text(text, encoding=\"utf-8\")\n",
    "    return str(target)\n",
    "\n",
    "def write_manifest(dataset_path: str | Path, meta: dict) -> str:\n",
    "    rd = run_dir_for(dataset_path)\n",
    "    p = rd / \"manifest.json\"\n",
    "    p.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    return str(p)\n",
    "\n",
    "def error_snippet(exc, max_lines=20, max_chars=1500):\n",
    "    tb = traceback.TracebackException.from_exception(exc, capture_locals=False)\n",
    "    frames = list(tb.format())\n",
    "    txt = \"\".join(frames[:5] + frames[-max_lines:]).strip()\n",
    "    if isinstance(exc, AssertionError) and \"invalid_entries\" in str(exc):\n",
    "        msg = str(exc)\n",
    "        truncated = msg[:500] + (\" …\" if len(msg) > 500 else \"\")\n",
    "        txt = truncated + \"\\n\\n\" + txt\n",
    "    return (txt[:max_chars] + \" …\") if len(txt) > max_chars else txt\n",
    "\n",
    "# Prompts\n",
    "def build_transformation_prompt(ctx, meta):\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    [ROLE]\n",
    "    You are an expert data scientist specialised in cleansing and standardising film datasets.\n",
    "    Try to interpret the schema and the context and the metadata to understand the data.\n",
    "\n",
    "    [PROCESS]\n",
    "      (a) [PLAN] Outline your high-level approach.\n",
    "      (b) [THINK] Write Pandas code step by step to fullfill the {SCHEMA_TEXT}.\n",
    "      (c) [CHECK] Self-verify logic.\n",
    "      (d) [ANSWER] return one ```python``` block that:\n",
    "          • starts with  output = []\n",
    "          • Select the most complete numeric rating, prefer original ratings, fallback if empty.\n",
    "          • preserve the column {ID_COL} unchanged in every output row as the ID column\n",
    "          • interprets two-digit years so that the final four-digit year falls in the realistic range 1900-2025\n",
    "          • genres: list of strings \n",
    "          • title: normalize exactly: ascii+lower; remove one trailing \"(YYYY)\" then one trailing \"(...)\"; punctuation→space; collapse spaces+trim; drop trailing \"the\"; dedup tokens (keep order).\n",
    "          • release_year: could also be \"streaming_release_year\", if necessary extract it, has to be between 1870 and 2025\n",
    "          • builds an additional list  invalid_entries\n",
    "            – append a dict whenever a row is skipped\n",
    "              {{ \"row\": <original_row_as_dict>, \"reason\": \"<short text>\" }}\n",
    "\n",
    "    [SCHEMA]\n",
    "    {SCHEMA_TEXT}\n",
    "\n",
    "    [CONTEXT]\n",
    "    {ctx}\n",
    "\n",
    "    [METADATA]\n",
    "    {meta}\n",
    "    \"\"\").strip()\n",
    "\n",
    "def build_validator_prompt():\n",
    "    schema_json = json.dumps(SCHEMA_SPEC, indent=2, ensure_ascii=False)\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    [ROLE]\n",
    "    Validation stage of the ETL pipeline.\n",
    "\n",
    "    [SCHEMA_JSON]\n",
    "    {schema_json}\n",
    "\n",
    "    [PROCESS]\n",
    "      (a) [PLAN] outline checks\n",
    "      (b) [THINK] derive assertions\n",
    "      (c) [CHECK] ensure logic\n",
    "      (d) [ANSWER] one ```python``` block that\n",
    "          • iterates **over `output` (list of dicts)**\n",
    "          • builds valid_output / invalid_entries / duplicate_count\n",
    "          • raises AssertionError if invalid_entries is non-empty.\n",
    "\n",
    "    [FORMAT] Use [PLAN] [THINK] [CHECK] [ANSWER].\n",
    "    [OUTPUT] show only code inside the block.\n",
    "    \"\"\").strip()\n",
    "\n",
    "# OpenAI\n",
    "def get_completion(prompt: str):\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY nicht gesetzt. Bitte als ENV setzen.\")\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "# Report\n",
    "def completeness_report(df0, out, v_llm, v_det, inv_det, savedir):\n",
    "    print(\"\\n— Completeness Report —\")\n",
    "    print(f\"Original rows:            {len(df0):>6}\")\n",
    "    print(f\"After transformation:     {len(out):>6}   (-{len(df0)-len(out)})\")\n",
    "    print(f\"LLM validator valid:      {len(v_llm):>6}   (-{len(out)-len(v_llm)})\")\n",
    "    if inv_det:\n",
    "        bad = pd.DataFrame(inv_det)\n",
    "        print(\"\\nBeispiele ungültiger Datensätze (dynamic):\")\n",
    "        print(bad.head().to_string(index=False))\n",
    "        rep = Path(savedir) / f\"{Path(DATA_PATH).stem}_invalid_records.csv\"\n",
    "        bad.to_csv(rep, index=False)\n",
    "        print(f\"\\n❌  Fehlerliste gespeichert unter: {rep}\")\n",
    "\n",
    "    # Kopie des Reports im Run-Ordner\n",
    "    lines = [\n",
    "        \"— Completeness Report —\",\n",
    "        f\"Original rows:            {len(df0):>6}\",\n",
    "        f\"After transformation:     {len(out):>6}   (-{len(df0)-len(out)})\",\n",
    "        f\"LLM validator valid:      {len(v_llm):>6}   (-{len(out)-len(v_llm)})\",\n",
    "    ]\n",
    "    (run_dir_for(DATA_PATH) / \"completeness.txt\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Running with pandas\", pd.__version__)\n",
    "        append_log(f\"RUN {RUN_ID} started with pandas {pd.__version__}\", DATA_PATH)\n",
    "\n",
    "        df = load_df(); globals()[\"df\"] = df\n",
    "        df[ID_COL] = range(1, len(df) + 1)\n",
    "        globals()[\"ID_COL\"] = ID_COL\n",
    "\n",
    "        ctx, meta = make_context_and_meta(df)\n",
    "\n",
    "        # Transformation (mit einmaligem Retry)\n",
    "        snippet = \"\"\n",
    "        manifest_paths = {\"transform\": {\"prompt\": [], \"code\": []},\n",
    "                          \"validator\": {\"prompt\": [], \"code\": []}}\n",
    "\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                prompt = build_transformation_prompt(ctx, meta)\n",
    "                if attempt == 1:\n",
    "                    prompt += f\"\\n[TRANSFORMATION_ERROR]\\n{snippet}\"\n",
    "\n",
    "                save_artifact(prompt, \"transform\", \"prompt\", DATA_PATH, attempt)\n",
    "                pth = save_artifact_copy(prompt, \"transform\", \"prompt\", DATA_PATH, attempt, add_hash=False)\n",
    "                manifest_paths[\"transform\"][\"prompt\"].append(pth)\n",
    "\n",
    "                t_code = get_completion(prompt)\n",
    "                save_artifact(extract_code_block(t_code), \"transform\", \"code\", DATA_PATH, attempt)\n",
    "                print(\"\\n— Transformation Code —\\n\", t_code)\n",
    "\n",
    "                t_code_block = extract_code_block(t_code)\n",
    "                pth = save_artifact_copy(t_code_block, \"transform\", \"code\", DATA_PATH, attempt, add_hash=True)\n",
    "                manifest_paths[\"transform\"][\"code\"].append(pth)\n",
    "\n",
    "                exec_generated_code(t_code, globals())\n",
    "                if \"output\" not in globals():\n",
    "                    raise RuntimeError(\"LLM produced no `output`\")\n",
    "\n",
    "                append_log(f\"[TRANSFORM][attempt={attempt}] OK, rows={len(globals()['output'])}\", DATA_PATH)\n",
    "                break\n",
    "\n",
    "            except Exception as exc:\n",
    "                snippet = error_snippet(exc)\n",
    "                print(\"⚠️  transformation crashed – retrying …\\n\", snippet)\n",
    "                append_log(f\"[TRANSFORM][attempt={attempt}] ERROR\\n{traceback.format_exc()}\", DATA_PATH)\n",
    "                globals().pop(\"output\", None)\n",
    "                if attempt == 1:\n",
    "                    raise\n",
    "\n",
    "        # Persist: Transform-Invalids\n",
    "        trans_invalid = globals().get(\"invalid_entries\", [])\n",
    "        trans_invalid_count = 0\n",
    "        if trans_invalid:\n",
    "            inv_df = pd.DataFrame(trans_invalid)\n",
    "            tr_inv_path = INVALID_DIR / f\"{Path(DATA_PATH).stem}_invalid.csv\"\n",
    "            inv_df.to_csv(tr_inv_path, index=False)\n",
    "            trans_invalid_count = len(inv_df)\n",
    "            print(f\"\\n❌  Transform dropped {trans_invalid_count} rows  →  {tr_inv_path}\")\n",
    "            append_log(f\"[TRANSFORM] dropped {trans_invalid_count} rows → {tr_inv_path}\", DATA_PATH)\n",
    "        globals().pop(\"invalid_entries\", None)\n",
    "\n",
    "        # Validator (mit Retry)\n",
    "        validator_prompt = build_validator_prompt()\n",
    "        save_artifact(validator_prompt, \"validator\", \"prompt\", DATA_PATH)\n",
    "        pth = save_artifact_copy(validator_prompt, \"validator\", \"prompt\", DATA_PATH, attempt=0, add_hash=False)\n",
    "        manifest_paths[\"validator\"][\"prompt\"].append(pth)\n",
    "\n",
    "        v_code = get_completion(validator_prompt)\n",
    "        save_artifact(extract_code_block(v_code), \"validator\", \"code\", DATA_PATH)\n",
    "        print(\"\\n— Validator Code —\\n\", v_code)\n",
    "        v_code_block = extract_code_block(v_code)\n",
    "        pth = save_artifact_copy(v_code_block, \"validator\", \"code\", DATA_PATH, attempt=0, add_hash=True)\n",
    "        manifest_paths[\"validator\"][\"code\"].append(pth)\n",
    "\n",
    "        try:\n",
    "            exec_generated_code(v_code, globals())  # setzt valid_output, invalid_entries …\n",
    "            append_log(f\"[VALIDATOR] OK, valid_output={len(globals().get('valid_output', []))}\", DATA_PATH)\n",
    "        except Exception as exc:\n",
    "            snippet = error_snippet(exc)\n",
    "            print(\"⚠️  validator crashed – retrying …\\n\", snippet)\n",
    "            append_log(f\"[VALIDATOR] ERROR\\n{traceback.format_exc()}\", DATA_PATH)\n",
    "\n",
    "            for var in (\"output\", \"valid_output\", \"invalid_entries\", \"duplicate_count\"):\n",
    "                globals().pop(var, None)\n",
    "\n",
    "            retry_prompt = (build_transformation_prompt(ctx, meta)\n",
    "                            + f\"\\n[VALIDATION_ERROR]\\n{snippet}\")\n",
    "            save_artifact(retry_prompt, \"transform\", \"prompt\", DATA_PATH, attempt=1)\n",
    "            pth = save_artifact_copy(retry_prompt, \"transform\", \"prompt\", DATA_PATH, attempt=1, add_hash=False)\n",
    "            manifest_paths[\"transform\"][\"prompt\"].append(pth)\n",
    "\n",
    "            t_code = get_completion(retry_prompt)\n",
    "            save_artifact(extract_code_block(t_code), \"transform\", \"code\", DATA_PATH, attempt=1)\n",
    "            print(\"\\n— Retry Transformation Code —\\n\", t_code)\n",
    "            t_code_block = extract_code_block(t_code)\n",
    "            pth = save_artifact_copy(t_code_block, \"transform\", \"code\", DATA_PATH, attempt=1, add_hash=True)\n",
    "            manifest_paths[\"transform\"][\"code\"].append(pth)\n",
    "\n",
    "            exec_generated_code(t_code, globals())\n",
    "            if \"output\" not in globals():\n",
    "                raise RuntimeError(\"Retry produced no `output`\")\n",
    "\n",
    "            validator_prompt_retry = build_validator_prompt()\n",
    "            save_artifact(validator_prompt_retry, \"validator\", \"prompt\", DATA_PATH, attempt=1)\n",
    "            pth = save_artifact_copy(validator_prompt_retry, \"validator\", \"prompt\", DATA_PATH, attempt=1, add_hash=False)\n",
    "            manifest_paths[\"validator\"][\"prompt\"].append(pth)\n",
    "\n",
    "            v_code = get_completion(validator_prompt_retry)\n",
    "            save_artifact(extract_code_block(v_code), \"validator\", \"code\", DATA_PATH, attempt=1)\n",
    "            print(\"\\n— Retry Validator Code —\\n\", v_code)\n",
    "            v_code_block = extract_code_block(v_code)\n",
    "            pth = save_artifact_copy(v_code_block, \"validator\", \"code\", DATA_PATH, attempt=1, add_hash=True)\n",
    "            manifest_paths[\"validator\"][\"code\"].append(pth)\n",
    "\n",
    "            exec_generated_code(v_code, globals())\n",
    "            append_log(f\"[VALIDATOR][retry] OK, valid_output={len(globals().get('valid_output', []))}\", DATA_PATH)\n",
    "\n",
    "        # Validator-Invalids persistieren\n",
    "        val_invalid = globals().get(\"invalid_entries\", [])\n",
    "        val_invalid_count = 0\n",
    "        if val_invalid:\n",
    "            val_inv_path = INVALID_DIR / f\"{Path(DATA_PATH).stem}_invalid_validator.csv\"\n",
    "            pd.DataFrame(val_invalid).to_csv(val_inv_path, index=False)\n",
    "            val_invalid_count = len(val_invalid)\n",
    "            print(f\"❌  Validator flagged {val_invalid_count} rows  →  {val_inv_path}\")\n",
    "            append_log(f\"[VALIDATOR] invalid rows: {val_invalid_count} → {val_inv_path}\", DATA_PATH)\n",
    "\n",
    "        # Report + Persist final\n",
    "        completeness_report(df, output, valid_output, valid_output, val_invalid, CLEAN_DIR)\n",
    "        out_path = CLEAN_DIR / Path(DATA_PATH).name\n",
    "        pd.DataFrame(valid_output).to_csv(out_path, index=False)\n",
    "        print(f\"\\n✅ Bereinigter Datensatz gespeichert unter: {out_path}\")\n",
    "        append_log(f\"[OUTPUT] written → {out_path}\", DATA_PATH)\n",
    "\n",
    "        # Manifest\n",
    "        manifest = {\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"dataset\": str(DATA_PATH),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"sample_size\": SAMPLE_SIZE,\n",
    "            \"rng_state\": RNG_STATE,\n",
    "            \"outputs\": {\n",
    "                \"cleaned_csv\": str(out_path),\n",
    "                \"transform_invalid_count\": trans_invalid_count,\n",
    "                \"validator_invalid_count\": val_invalid_count\n",
    "            },\n",
    "            \"artifacts\": manifest_paths\n",
    "        }\n",
    "        man_path = write_manifest(DATA_PATH, manifest)\n",
    "        append_log(f\"[MANIFEST] {man_path}\", DATA_PATH)\n",
    "\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        append_log(f\"[FATAL] {traceback.format_exc()}\", DATA_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40804f",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d65be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re, textwrap, datetime as dt\n",
    "\n",
    "try:\n",
    "    from unidecode import unidecode   # Akzente → ASCII\n",
    "except ImportError:\n",
    "    unidecode = lambda s: s\n",
    "\n",
    "# Eingabe-/Ausgabe\n",
    "CLEAN_DIR = Path(\"cleaned\")\n",
    "MERGE_OUT = Path(\"merged/all_movies_wide_fuzzy.csv\")\n",
    "DUPL_OUT  = Path(\"merged/all_movies_fuzzy_duplicates.csv\")\n",
    "\n",
    "# Kanonische Rating-Spalten je Quelle\n",
    "CANON = {\n",
    "    \"imdb_data\":              \"rating_imdb\",\n",
    "    \"movielens_aggregated\":   \"rating_movielens\",\n",
    "    \"metacritic_movies\":      \"rating_metacritic\",\n",
    "    \"rotten_tomatoes_movies\": \"rating_rt_audience\",\n",
    "}\n",
    "\n",
    "# Kanonische ID-Spalten je Quelle\n",
    "ID_MAP = {\n",
    "    \"imdb_data\":              \"ID_IMDB\",\n",
    "    \"movielens_aggregated\":   \"ID_MOVIELENS\",\n",
    "    \"metacritic_movies\":      \"ID_METACRITIC\",\n",
    "    \"rotten_tomatoes_movies\": \"ID_RT\",\n",
    "}\n",
    "\n",
    "def standardize_id_column(df: pd.DataFrame, src: str) -> pd.DataFrame:\n",
    "    target = ID_MAP.get(src, f\"ID_{src.upper()}\")\n",
    "    if target in df.columns:\n",
    "        pass\n",
    "    elif \"ID\" in df.columns:\n",
    "        df = df.rename(columns={\"ID\": target})\n",
    "    else:\n",
    "        cands = [c for c in df.columns if str(c).upper().startswith(\"ID_\")]\n",
    "        if cands:\n",
    "            df = df.rename(columns={cands[0]: target})\n",
    "    if target in df.columns:\n",
    "        df[target] = pd.to_numeric(df[target], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "# 1) Clean-CSVs einsammeln\n",
    "frames = []\n",
    "for csv in CLEAN_DIR.glob(\"*.csv\"):\n",
    "    name = csv.name\n",
    "    if (\n",
    "        name.endswith((\"merged.csv\", \"all_movies_wide.csv\", \"all_movies_wide_fuzzy.csv\"))\n",
    "        or \"_invalid_records\" in name\n",
    "        or \"_duplicates\"      in name\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    src = csv.stem  # imdb_data, movielens_aggregated, …\n",
    "    df  = pd.read_csv(csv)\n",
    "\n",
    "    # Jahr vereinheitlichen\n",
    "    if \"year\" in df.columns and \"release_year\" not in df.columns:\n",
    "        df = df.rename(columns={\"year\": \"release_year\"})\n",
    "\n",
    "    # Rating in kanonische Spalte umbenennen (falls nötig)\n",
    "    rating_col = CANON.get(src, f\"rating_{src}\")\n",
    "    if \"rating\" in df.columns and rating_col not in df.columns:\n",
    "        df = df.rename(columns={\"rating\": rating_col})\n",
    "\n",
    "    # ID-Spalte standardisieren\n",
    "    df = standardize_id_column(df, src)\n",
    "\n",
    "    df[\"source\"] = src\n",
    "    frames.append(df)\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(f\"Keine geeigneten Clean-CSVs in {CLEAN_DIR}\")\n",
    "\n",
    "long_df = pd.concat(frames, ignore_index=True)\n",
    "long_df[\"norm_title\"]   = long_df[\"title\"].map(norm_title)\n",
    "long_df[\"release_year\"] = pd.to_numeric(long_df[\"release_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 3) Jahr-Cluster (±1) – wie statisch\n",
    "def year_cluster(sub: pd.DataFrame) -> pd.Series:\n",
    "    years = sorted(set([y for y in sub[\"release_year\"].dropna()]))\n",
    "    clusters, cid = {}, 0\n",
    "    for y in years:\n",
    "        if any(abs(y - c) <= 1 for c in clusters.get(cid, [])):\n",
    "            clusters[cid].append(y)\n",
    "        else:\n",
    "            cid += 1; clusters[cid] = [y]\n",
    "    mapping = {y: c for c, ys in clusters.items() for y in ys}\n",
    "    return sub[\"release_year\"].map(mapping).fillna(cid + 1).astype(int)\n",
    "\n",
    "long_df[\"year_cluster\"] = (\n",
    "    long_df.groupby(\"norm_title\", group_keys=False)\n",
    "           .apply(year_cluster)\n",
    ")\n",
    "\n",
    "# 4) Helper für gruppierte Aggregation\n",
    "def first_valid(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if not s.empty else np.nan\n",
    "\n",
    "def first_non_empty(series: pd.Series):\n",
    "    for val in series:\n",
    "        if isinstance(val, list) and val:\n",
    "            return val\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val\n",
    "    return np.nan\n",
    "\n",
    "group_cols = [\"norm_title\", \"year_cluster\"]\n",
    "rating_cols = [c for c in long_df.columns if c.startswith(\"rating_\")]\n",
    "id_cols     = [c for c in long_df.columns if str(c).startswith(\"ID_\")]\n",
    "\n",
    "# 5) Ratings je Film (erste gültige) – entspricht statischem Pivot-Resultat\n",
    "ratings_wide = (\n",
    "    long_df.groupby(group_cols, as_index=False)[rating_cols]\n",
    "           .agg(first_valid) if rating_cols else long_df[group_cols].drop_duplicates()\n",
    ")\n",
    "\n",
    "# IDs je Film (erste gültige)\n",
    "ids_map = (\n",
    "    long_df.groupby(group_cols, as_index=False)[id_cols]\n",
    "           .agg(first_valid) if id_cols else long_df[group_cols].drop_duplicates()\n",
    ")\n",
    "\n",
    "# 6) Repräsentativer Titel & kleinstes Jahr\n",
    "meta = (\n",
    "    long_df.sort_values([\"source\", \"title\"])\n",
    "           .groupby(group_cols, as_index=False)\n",
    "           .agg(title=(\"title\", \"first\"), release_year=(\"release_year\", \"min\"))\n",
    ")\n",
    "\n",
    "# 7) Genres: erste nicht-leere Liste (wie statisch)\n",
    "genres_map = (\n",
    "    long_df.sort_values(\"source\")\n",
    "           .groupby(group_cols)[\"genres\"]\n",
    "           .apply(first_non_empty)\n",
    "           .reset_index(name=\"genres\")\n",
    ")\n",
    "\n",
    "# 8) Wide zusammenführen\n",
    "wide = ratings_wide.merge(meta,       on=group_cols, how=\"left\")\n",
    "wide = wide.merge(genres_map,         on=group_cols, how=\"left\")\n",
    "wide = wide.merge(ids_map,            on=group_cols, how=\"left\")\n",
    "\n",
    "# 9) Ungefiltertes Ergebnis speichern\n",
    "MERGE_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "UNFILTERED_OUT = Path(\"merged/all_movies.csv\")\n",
    "wide.to_csv(UNFILTERED_OUT, index=False)\n",
    "print(f\"💾 Ungefilterter Wide-Frame gespeichert: {UNFILTERED_OUT}\")\n",
    "\n",
    "# 10) ≥2 Ratings-Filter (wie statisch)\n",
    "present_rating_mask = wide[rating_cols].notna().sum(axis=1) if rating_cols else pd.Series(0, index=wide.index)\n",
    "wide_filtered = wide.loc[present_rating_mask >= 2].copy()\n",
    "\n",
    "# 11) Finale Spalten (IDs nach vorne)\n",
    "ordered_cols = (id_cols + [\"title\", \"release_year\", \"genres\"] + rating_cols)\n",
    "ordered_cols = [c for c in ordered_cols if c in wide_filtered.columns]\n",
    "wide_filtered = wide_filtered[ordered_cols]\n",
    "\n",
    "# 12) Fuzzy-Duplikate (title, release_year)\n",
    "dup_mask   = wide_filtered.duplicated(subset=[\"title\", \"release_year\"], keep=False)\n",
    "duplicates = wide_filtered[dup_mask].copy()\n",
    "uniques    = wide_filtered[~dup_mask].copy()\n",
    "\n",
    "# 13) Endergebnisse speichern\n",
    "uniques.to_csv(MERGE_OUT, index=False)\n",
    "duplicates.to_csv(DUPL_OUT, index=False)\n",
    "\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "  — Fuzzy-Merge abgeschlossen ({dt.date.today()}) —\n",
    "  Eingelesene Quellen : {len(frames)}\n",
    "  Long-Records        : {len(long_df)}\n",
    "  Filme mit ≥2 Votes  : {len(uniques)}\n",
    "  Fuzzy-Duplikate     : {len(duplicates)}\n",
    "  👉 Gemergt  : {MERGE_OUT}\n",
    "  👉 Duplikate: {DUPL_OUT}\n",
    "\"\"\").strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6075eaf",
   "metadata": {},
   "source": [
    "Superscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Superscore (0-10 Skala, ungewichtet – wie static_pipeline) ──────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap, datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "MERGED      = Path(\"merged/all_movies_wide_fuzzy.csv\")\n",
    "OUT_CSV     = Path(\"merged/all_movies_superscore_adaptive.csv\")\n",
    "MIN_RATINGS = 2\n",
    "\n",
    "df = pd.read_csv(MERGED)\n",
    "\n",
    "# ID-Spalten erkennen und sauber typisieren (bleiben bis zum Ende erhalten)\n",
    "id_cols = [c for c in df.columns if c == \"ID\" or str(c).startswith(\"ID_\")]\n",
    "for c in id_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Rating-Spalten finden\n",
    "rating_cols = [c for c in df.columns if c.startswith(\"rating_\")]\n",
    "if not rating_cols:\n",
    "    raise RuntimeError(\"Keine rating_*-Spalten gefunden.\")\n",
    "\n",
    "# 1. Quelle → 0-10-Normierung (heuristisch wie in static_pipeline)\n",
    "def to_0_10(series):\n",
    "    if series.dropna().empty:\n",
    "        return series\n",
    "    mx = series.max()\n",
    "    if mx <= 5.5:      # 0–5 Skala → *2 (MovieLens)\n",
    "        return series * 2\n",
    "    if mx > 10:        # 0–100 Skala → /10 (Metacritic, RT%)\n",
    "        return series / 10\n",
    "    return series      # 0–10 unverändert (IMDb)\n",
    "\n",
    "norm_cols = []\n",
    "for col in rating_cols:\n",
    "    ncol = col.replace(\"rating_\", \"\") + \"_norm\"\n",
    "    df[ncol] = to_0_10(df[col])\n",
    "    norm_cols.append(ncol)\n",
    "\n",
    "# 2. Anzahl verfügbarer normalisierter Ratings\n",
    "df[\"num_available_ratings\"] = df[norm_cols].notna().sum(axis=1)\n",
    "\n",
    "# 3. Superscore (nur wenn ≥ MIN_RATINGS normalisierte Ratings vorhanden)\n",
    "mask = df[\"num_available_ratings\"] >= MIN_RATINGS\n",
    "df.loc[mask, \"superscore_mean_0_10\"]   = df.loc[mask, norm_cols].mean(axis=1).round(1)\n",
    "df.loc[mask, \"superscore_median_0_10\"] = df.loc[mask, norm_cols].median(axis=1).round(1)\n",
    "\n",
    "# 4. Ausgabe – ID-Spalten vorne\n",
    "base_cols = [c for c in [\"title\", \"release_year\", \"genres\"] if c in df.columns]\n",
    "tail_cols = [\"num_available_ratings\", \"superscore_mean_0_10\", \"superscore_median_0_10\"]\n",
    "cols_out  = id_cols + base_cols + rating_cols + norm_cols + tail_cols\n",
    "cols_out  = [c for c in cols_out if c in df.columns]\n",
    "\n",
    "df[cols_out].to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "  — Superscore-Static abgeschlossen ({dt.date.today()}) —\n",
    "  Rating-Spalten erkannt : {rating_cols}\n",
    "  Normierte Spalten      : {norm_cols}\n",
    "  Filme mit ≥{MIN_RATINGS} Ratings : {int(mask.sum())} / {len(df)}\n",
    "  ID-Spalten im Output   : {id_cols}\n",
    "  👉 Ergebnis : {OUT_CSV}\n",
    "\"\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecfbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
