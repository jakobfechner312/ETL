{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbe1e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with pandas 2.3.2\n",
      "\n",
      "— Transformation Code —\n",
      " ```python\n",
      "import pandas as pd\n",
      "import re\n",
      "import unicodedata\n",
      "\n",
      "output = []\n",
      "invalid_entries = []\n",
      "seen_title_year = set()\n",
      "\n",
      "for _, row in df.iterrows():\n",
      "    orig = row.to_dict()\n",
      "    # 1) Preserve MovieLens ID\n",
      "    movie_id = row[\"ID_MOVIELENS\"]\n",
      "\n",
      "    # 2) Determine release_year\n",
      "    year = None\n",
      "    # a) Prefer explicit column if it exists\n",
      "    if \"release_year\" in row and pd.notna(row[\"release_year\"]):\n",
      "        year = int(row[\"release_year\"])\n",
      "    else:\n",
      "        # b) Fallback: extract the last (...) in title, interpret 2‐digit → 1900+\n",
      "        m = re.search(r\"\\((\\d{2,4})\\)\\s*$\", row[\"title\"])\n",
      "        if m:\n",
      "            y = m.group(1)\n",
      "            year = int(y) if len(y) == 4 else 1900 + int(y)\n",
      "    if year is None:\n",
      "        invalid_entries.append({\"row\": orig, \"reason\": \"no_release_year\"})\n",
      "        continue\n",
      "    if not (1870 <= year <= 2025):\n",
      "        invalid_entries.append({\"row\": orig, \"reason\": \"invalid_year\"})\n",
      "        continue\n",
      "\n",
      "    # 3) Select rating (must not be NaN)\n",
      "    rating = row.get(\"average_rating\", pd.NA)\n",
      "    if pd.isna(rating):\n",
      "        invalid_entries.append({\"row\": orig, \"reason\": \"missing_rating\"})\n",
      "        continue\n",
      "\n",
      "    # 4) Clean title\n",
      "    t = row[\"title\"]\n",
      "    # a) remove one trailing (YYYY) then one more trailing (...), if present\n",
      "    t = re.sub(r\"\\(\\d{4}\\)\\s*$\", \"\", t)\n",
      "    t = re.sub(r\"\\([^)]*\\)\\s*$\", \"\", t)\n",
      "    # b) normalize to ASCII lowercase\n",
      "    t = unicodedata.normalize(\"NFKD\", t).encode(\"ascii\", \"ignore\").decode(\"ascii\").lower()\n",
      "    # c) punctuation→space, collapse spaces, strip\n",
      "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
      "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
      "    # d) drop trailing \" the\"\n",
      "    if t.endswith(\" the\"):\n",
      "        t = t[: -len(\" the\")].strip()\n",
      "    # e) dedupe tokens preserving order\n",
      "    tokens = []\n",
      "    for w in t.split():\n",
      "        if w not in tokens:\n",
      "            tokens.append(w)\n",
      "    title_clean = \" \".join(tokens)\n",
      "    # f) enforce uniqueness of (title, year)\n",
      "    key = (title_clean, year)\n",
      "    if key in seen_title_year:\n",
      "        invalid_entries.append({\"row\": orig, \"reason\": \"duplicate_title_year\"})\n",
      "        continue\n",
      "    seen_title_year.add(key)\n",
      "\n",
      "    # 5) Parse genres as list[str]\n",
      "    g = row.get(\"genres\", \"\")\n",
      "    if pd.isna(g) or g.strip().lower() in (\"\", \"(no genres listed)\"):\n",
      "        genres_list = []\n",
      "    else:\n",
      "        genres_list = [x for x in g.split(\"|\") if x]\n",
      "\n",
      "    # 6) Append cleaned entry\n",
      "    output.append({\n",
      "        \"ID\": movie_id,\n",
      "        \"title\": title_clean,\n",
      "        \"release_year\": year,\n",
      "        \"genres\": genres_list,\n",
      "        \"rating\": float(rating)\n",
      "    })\n",
      "```\n",
      "\n",
      "❌  Transform dropped 18 rows  →  /Users/jakob/Desktop/BachlelorThesis/Bachelor_final/ETL/adaptive/data/invalide/movielens_aggregated_invalid.csv\n",
      "\n",
      "— Validator Code —\n",
      " [PLAN]\n",
      "1. Verify each row’s fields per schema:\n",
      "   - ID: present, int.\n",
      "   - title: non-empty str, cleaned (lowercase, alphanumeric+spaces), unique with release_year.\n",
      "   - release_year: int in [1870,2025] or pd.NA.\n",
      "   - genres: if present, list of str.\n",
      "   - rating: float, not NaN.\n",
      "2. Track seen (title, release_year) to count duplicates.\n",
      "3. Collect valid rows, invalid entries, duplicate count.\n",
      "4. Raise AssertionError if any invalid entries.\n",
      "\n",
      "[THINK]\n",
      "- Use pandas.isna to detect pd.NA/NaN.\n",
      "- Clean title via regex: remove non-alphanum/spaces, collapse spaces.\n",
      "- Uniqueness on cleaned title + release_year.\n",
      "- Append validated dicts to valid_output; skip duplicates.\n",
      "- Gather invalid entries with index and error messages.\n",
      "\n",
      "[CHECK]\n",
      "- Ensure all branches covered.\n",
      "- Validate types and ranges correctly.\n",
      "- Duplicate logic increments count and skips row.\n",
      "- Assertion error includes invalid_entries detail.\n",
      "\n",
      "[ANSWER]\n",
      "```python\n",
      "import re\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "valid_output = []\n",
      "invalid_entries = []\n",
      "seen_titles = set()\n",
      "duplicate_count = 0\n",
      "\n",
      "for idx, row in enumerate(output):\n",
      "    errors = []\n",
      "    validated = {}\n",
      "\n",
      "    # ID\n",
      "    id_val = row.get(\"ID\")\n",
      "    if not isinstance(id_val, int):\n",
      "        errors.append(\"ID must be int\")\n",
      "    else:\n",
      "        validated[\"ID\"] = id_val\n",
      "\n",
      "    # title\n",
      "    title = row.get(\"title\")\n",
      "    if not isinstance(title, str) or not title.strip():\n",
      "        errors.append(\"title must be non-empty str\")\n",
      "    else:\n",
      "        cleaned = title.lower()\n",
      "        cleaned = re.sub(r\"[^a-z0-9\\s]\", \"\", cleaned)\n",
      "        cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
      "        if not cleaned:\n",
      "            errors.append(\"title cleaned to empty\")\n",
      "        else:\n",
      "            validated[\"title\"] = cleaned\n",
      "\n",
      "    # release_year\n",
      "    ry = row.get(\"release_year\")\n",
      "    if pd.isna(ry):\n",
      "        validated[\"release_year\"] = pd.NA\n",
      "    elif isinstance(ry, (int, np.integer)):\n",
      "        if 1870 <= int(ry) <= 2025:\n",
      "            validated[\"release_year\"] = int(ry)\n",
      "        else:\n",
      "            errors.append(\"release_year out of realistic range\")\n",
      "    else:\n",
      "        errors.append(\"release_year must be int or NA\")\n",
      "\n",
      "    # genres\n",
      "    genres = row.get(\"genres\", None)\n",
      "    if genres is None:\n",
      "        validated[\"genres\"] = []\n",
      "    elif isinstance(genres, list) and all(isinstance(g, str) for g in genres):\n",
      "        validated[\"genres\"] = genres\n",
      "    else:\n",
      "        errors.append(\"genres must be list[str] if present\")\n",
      "\n",
      "    # rating\n",
      "    rating = row.get(\"rating\")\n",
      "    if rating is None or pd.isna(rating):\n",
      "        errors.append(\"rating must not be NaN\")\n",
      "    else:\n",
      "        try:\n",
      "            validated[\"rating\"] = float(rating)\n",
      "        except Exception:\n",
      "            errors.append(\"rating must be convertable to float\")\n",
      "\n",
      "    # finalize\n",
      "    if errors:\n",
      "        invalid_entries.append({\"index\": idx, \"errors\": errors, \"row\": row})\n",
      "    else:\n",
      "        key = (validated[\"title\"], validated[\"release_year\"])\n",
      "        if key in seen_titles:\n",
      "            duplicate_count += 1\n",
      "        else:\n",
      "            seen_titles.add(key)\n",
      "            valid_output.append(validated)\n",
      "\n",
      "assert not invalid_entries, f\"Invalid entries found: {invalid_entries}\"\n",
      "```\n",
      "\n",
      "— Completeness Report —\n",
      "Original rows:              9724\n",
      "After transformation:       9706   (-18)\n",
      "LLM validator valid:        9706   (-0)\n",
      "\n",
      "✅ Bereinigter Datensatz gespeichert unter: /Users/jakob/Desktop/BachlelorThesis/Bachelor_final/ETL/adaptive/cleaned/movielens_aggregated.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import hashlib\n",
    "import textwrap\n",
    "import traceback\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from unidecode import unidecode  # ggf. von LLM-Code genutzt\n",
    "\n",
    "# Pfade robust (Notebook oder Skript)\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "# Verzeichnisse\n",
    "CLEAN_DIR    = BASE_DIR / \"cleaned\"\n",
    "RAW_DIR      = BASE_DIR / \"data\" / \"raw\"\n",
    "ARTIFACT_DIR = BASE_DIR / \"run_artifacts\"\n",
    "INVALID_DIR  = BASE_DIR / \"data\" / \"invalide\"\n",
    "for d in (CLEAN_DIR, ARTIFACT_DIR, INVALID_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Konfiguration\n",
    "DATA_PATH  = RAW_DIR / \"/Users/jakob/Desktop/BachlelorThesis/Bachelor_final/ETL/adaptive/data/raw/movielens_aggregated.csv\"  # bei Bedarf auf andere Quelle setzen\n",
    "MODEL_NAME = \"o4-mini\"\n",
    "SAMPLE_SIZE = 5\n",
    "RNG_STATE   = 42\n",
    "\n",
    "# ENV\n",
    "load_dotenv(BASE_DIR.parent / \".env\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Laufkontext\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Schema\n",
    "SCHEMA_PATH = BASE_DIR / \"schema.json\"\n",
    "SCHEMA_SPEC = json.loads(SCHEMA_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Helper\n",
    "def schema_as_text(spec: list[dict]) -> str:\n",
    "    return \"\\n\".join(f\"- {d['field']}: (type: {d['type']}) {d['rule']}\" for d in spec)\n",
    "\n",
    "def infer_dataset_tag(path: str | Path) -> str:\n",
    "    name = Path(path).name.lower()\n",
    "    if \"imdb\" in name: return \"IMDB\"\n",
    "    if \"movielens\" in name: return \"MOVIELENS\"\n",
    "    if \"metacritic\" in name: return \"METACRITIC\"\n",
    "    if \"rotten\" in name or \"rt\" in name: return \"RT\"\n",
    "    return \"DATA\"\n",
    "\n",
    "DATASET_TAG = infer_dataset_tag(DATA_PATH)\n",
    "ID_COL = f\"ID_{DATASET_TAG}\"\n",
    "SCHEMA_TEXT = schema_as_text(SCHEMA_SPEC)\n",
    "\n",
    "def load_df(path=DATA_PATH):\n",
    "    return pd.read_csv(path, on_bad_lines=\"skip\")\n",
    "\n",
    "def make_context_and_meta(df):\n",
    "    ctx = df.sample(SAMPLE_SIZE, random_state=RNG_STATE).to_string(index=False)\n",
    "    buf = StringIO(); df.info(buf=buf)\n",
    "    return ctx, buf.getvalue()\n",
    "\n",
    "def extract_code_block(md: str) -> str:\n",
    "    if \"```\" in md:\n",
    "        md = md.split(\"```\")[-2]\n",
    "    return md.replace(\"python\", \"\", 1).replace(\"```\", \"\").strip()\n",
    "\n",
    "def exec_generated_code(code: str, g: dict):\n",
    "    block = extract_code_block(code) if \"```\" in code else code\n",
    "    exec(block, g)\n",
    "\n",
    "# Artefakte & Logging\n",
    "def run_dir_for(dataset_path: str | Path) -> Path:\n",
    "    stem = Path(dataset_path).stem\n",
    "    d = ARTIFACT_DIR / f\"{stem}_{RUN_ID}\"\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def append_log(msg: str, dataset_path: str | Path, fname: str = \"run.log\"):\n",
    "    rd = run_dir_for(dataset_path)\n",
    "    with open(rd / fname, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg.rstrip() + \"\\n\")\n",
    "\n",
    "def save_artifact(text: str, stage: str, kind: str, dataset_path: str | Path, attempt: int = 0):\n",
    "    stem = Path(dataset_path).stem\n",
    "    suffix = \"_retry\" if attempt else \"\"\n",
    "    ext = \"txt\" if kind == \"prompt\" else \"py\"\n",
    "    target_dir = ARTIFACT_DIR / stem\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (target_dir / f\"{stage}_{kind}{suffix}.{ext}\").write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "def save_artifact_copy(text: str, stage: str, kind: str, dataset_path: str | Path,\n",
    "                       attempt: int = 0, add_hash: bool = False) -> str:\n",
    "    run_dir = run_dir_for(dataset_path)\n",
    "    suffix = \"_retry\" if attempt else \"\"\n",
    "    ext = \"txt\" if kind == \"prompt\" else \"py\"\n",
    "    fname = f\"{stage}_{kind}{suffix}\"\n",
    "    if add_hash:\n",
    "        digest = hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:12]\n",
    "        fname += f\"_{digest}\"\n",
    "    target = run_dir / f\"{fname}.{ext}\"\n",
    "    target.write_text(text, encoding=\"utf-8\")\n",
    "    return str(target)\n",
    "\n",
    "def write_manifest(dataset_path: str | Path, meta: dict) -> str:\n",
    "    rd = run_dir_for(dataset_path)\n",
    "    p = rd / \"manifest.json\"\n",
    "    p.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    return str(p)\n",
    "\n",
    "def error_snippet(exc, max_lines=20, max_chars=1500):\n",
    "    tb = traceback.TracebackException.from_exception(exc, capture_locals=False)\n",
    "    frames = list(tb.format())\n",
    "    txt = \"\".join(frames[:5] + frames[-max_lines:]).strip()\n",
    "    if isinstance(exc, AssertionError) and \"invalid_entries\" in str(exc):\n",
    "        msg = str(exc)\n",
    "        truncated = msg[:500] + (\" …\" if len(msg) > 500 else \"\")\n",
    "        txt = truncated + \"\\n\\n\" + txt\n",
    "    return (txt[:max_chars] + \" …\") if len(txt) > max_chars else txt\n",
    "\n",
    "# Prompts\n",
    "def build_transformation_prompt(ctx, meta):\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    [ROLE]\n",
    "    You are an expert data scientist specialised in cleansing and standardising film datasets.\n",
    "    Try to interpret the schema and the context and the metadata to understand the data.\n",
    "\n",
    "    [PROCESS]\n",
    "      (a) [PLAN] Outline your high-level approach.\n",
    "      (b) [THINK] Write Pandas code step by step to fullfill the {SCHEMA_TEXT}.\n",
    "      (c) [CHECK] Self-verify logic.\n",
    "      (d) [ANSWER] return one ```python``` block that:\n",
    "          • starts with  output = []\n",
    "          • Select the most complete numeric rating, prefer original ratings, fallback if empty.\n",
    "          • preserve the column {ID_COL} unchanged in every output row as the ID column\n",
    "          • interprets two-digit years so that the final four-digit year falls in the realistic range 1900-2025\n",
    "          • genres: list of strings \n",
    "          • title: normalize exactly: ascii+lower; remove one trailing \"(YYYY)\" then one trailing \"(...)\"; punctuation→space; collapse spaces+trim; drop trailing \"the\"; dedup tokens (keep order).\n",
    "          • release_year: could also be \"streaming_release_year\", if necessary extract it, has to be between 1870 and 2025\n",
    "          • builds an additional list  invalid_entries\n",
    "            – append a dict whenever a row is skipped\n",
    "              {{ \"row\": <original_row_as_dict>, \"reason\": \"<short text>\" }}\n",
    "\n",
    "    [SCHEMA]\n",
    "    {SCHEMA_TEXT}\n",
    "\n",
    "    [CONTEXT]\n",
    "    {ctx}\n",
    "\n",
    "    [METADATA]\n",
    "    {meta}\n",
    "    \"\"\").strip()\n",
    "\n",
    "def build_validator_prompt():\n",
    "    schema_json = json.dumps(SCHEMA_SPEC, indent=2, ensure_ascii=False)\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    [ROLE]\n",
    "    Validation stage of the ETL pipeline.\n",
    "\n",
    "    [SCHEMA_JSON]\n",
    "    {schema_json}\n",
    "\n",
    "    [PROCESS]\n",
    "      (a) [PLAN] outline checks\n",
    "      (b) [THINK] derive assertions\n",
    "      (c) [CHECK] ensure logic\n",
    "      (d) [ANSWER] one ```python``` block that\n",
    "          • iterates **over `output` (list of dicts)**\n",
    "          • builds valid_output / invalid_entries / duplicate_count\n",
    "          • raises AssertionError if invalid_entries is non-empty.\n",
    "\n",
    "    [FORMAT] Use [PLAN] [THINK] [CHECK] [ANSWER].\n",
    "    [OUTPUT] show only code inside the block.\n",
    "    \"\"\").strip()\n",
    "\n",
    "# OpenAI\n",
    "def get_completion(prompt: str):\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY nicht gesetzt. Bitte als ENV setzen.\")\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "# Report\n",
    "def completeness_report(df0, out, v_llm, v_det, inv_det, savedir):\n",
    "    print(\"\\n— Completeness Report —\")\n",
    "    print(f\"Original rows:            {len(df0):>6}\")\n",
    "    print(f\"After transformation:     {len(out):>6}   (-{len(df0)-len(out)})\")\n",
    "    print(f\"LLM validator valid:      {len(v_llm):>6}   (-{len(out)-len(v_llm)})\")\n",
    "    if inv_det:\n",
    "        bad = pd.DataFrame(inv_det)\n",
    "        print(\"\\nBeispiele ungültiger Datensätze (dynamic):\")\n",
    "        print(bad.head().to_string(index=False))\n",
    "        rep = Path(savedir) / f\"{Path(DATA_PATH).stem}_invalid_records.csv\"\n",
    "        bad.to_csv(rep, index=False)\n",
    "        print(f\"\\n❌  Fehlerliste gespeichert unter: {rep}\")\n",
    "\n",
    "    # Kopie des Reports im Run-Ordner\n",
    "    lines = [\n",
    "        \"— Completeness Report —\",\n",
    "        f\"Original rows:            {len(df0):>6}\",\n",
    "        f\"After transformation:     {len(out):>6}   (-{len(df0)-len(out)})\",\n",
    "        f\"LLM validator valid:      {len(v_llm):>6}   (-{len(out)-len(v_llm)})\",\n",
    "    ]\n",
    "    (run_dir_for(DATA_PATH) / \"completeness.txt\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Running with pandas\", pd.__version__)\n",
    "        append_log(f\"RUN {RUN_ID} started with pandas {pd.__version__}\", DATA_PATH)\n",
    "\n",
    "        df = load_df(); globals()[\"df\"] = df\n",
    "        df[ID_COL] = range(1, len(df) + 1)\n",
    "        globals()[\"ID_COL\"] = ID_COL\n",
    "\n",
    "        ctx, meta = make_context_and_meta(df)\n",
    "\n",
    "        # Transformation (mit einmaligem Retry)\n",
    "        snippet = \"\"\n",
    "        manifest_paths = {\"transform\": {\"prompt\": [], \"code\": []},\n",
    "                          \"validator\": {\"prompt\": [], \"code\": []}}\n",
    "\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                prompt = build_transformation_prompt(ctx, meta)\n",
    "                if attempt == 1:\n",
    "                    prompt += f\"\\n[TRANSFORMATION_ERROR]\\n{snippet}\"\n",
    "\n",
    "                save_artifact(prompt, \"transform\", \"prompt\", DATA_PATH, attempt)\n",
    "                pth = save_artifact_copy(prompt, \"transform\", \"prompt\", DATA_PATH, attempt, add_hash=False)\n",
    "                manifest_paths[\"transform\"][\"prompt\"].append(pth)\n",
    "\n",
    "                t_code = get_completion(prompt)\n",
    "                save_artifact(extract_code_block(t_code), \"transform\", \"code\", DATA_PATH, attempt)\n",
    "                print(\"\\n— Transformation Code —\\n\", t_code)\n",
    "\n",
    "                t_code_block = extract_code_block(t_code)\n",
    "                pth = save_artifact_copy(t_code_block, \"transform\", \"code\", DATA_PATH, attempt, add_hash=True)\n",
    "                manifest_paths[\"transform\"][\"code\"].append(pth)\n",
    "\n",
    "                exec_generated_code(t_code, globals())\n",
    "                if \"output\" not in globals():\n",
    "                    raise RuntimeError(\"LLM produced no `output`\")\n",
    "\n",
    "                append_log(f\"[TRANSFORM][attempt={attempt}] OK, rows={len(globals()['output'])}\", DATA_PATH)\n",
    "                break\n",
    "\n",
    "            except Exception as exc:\n",
    "                snippet = error_snippet(exc)\n",
    "                print(\"⚠️  transformation crashed – retrying …\\n\", snippet)\n",
    "                append_log(f\"[TRANSFORM][attempt={attempt}] ERROR\\n{traceback.format_exc()}\", DATA_PATH)\n",
    "                globals().pop(\"output\", None)\n",
    "                if attempt == 1:\n",
    "                    raise\n",
    "\n",
    "        # Persist: Transform-Invalids\n",
    "        trans_invalid = globals().get(\"invalid_entries\", [])\n",
    "        trans_invalid_count = 0\n",
    "        if trans_invalid:\n",
    "            inv_df = pd.DataFrame(trans_invalid)\n",
    "            tr_inv_path = INVALID_DIR / f\"{Path(DATA_PATH).stem}_invalid.csv\"\n",
    "            inv_df.to_csv(tr_inv_path, index=False)\n",
    "            trans_invalid_count = len(inv_df)\n",
    "            print(f\"\\n❌  Transform dropped {trans_invalid_count} rows  →  {tr_inv_path}\")\n",
    "            append_log(f\"[TRANSFORM] dropped {trans_invalid_count} rows → {tr_inv_path}\", DATA_PATH)\n",
    "        globals().pop(\"invalid_entries\", None)\n",
    "\n",
    "        # Validator (mit Retry)\n",
    "        validator_prompt = build_validator_prompt()\n",
    "        save_artifact(validator_prompt, \"validator\", \"prompt\", DATA_PATH)\n",
    "        pth = save_artifact_copy(validator_prompt, \"validator\", \"prompt\", DATA_PATH, attempt=0, add_hash=False)\n",
    "        manifest_paths[\"validator\"][\"prompt\"].append(pth)\n",
    "\n",
    "        v_code = get_completion(validator_prompt)\n",
    "        save_artifact(extract_code_block(v_code), \"validator\", \"code\", DATA_PATH)\n",
    "        print(\"\\n— Validator Code —\\n\", v_code)\n",
    "        v_code_block = extract_code_block(v_code)\n",
    "        pth = save_artifact_copy(v_code_block, \"validator\", \"code\", DATA_PATH, attempt=0, add_hash=True)\n",
    "        manifest_paths[\"validator\"][\"code\"].append(pth)\n",
    "\n",
    "        try:\n",
    "            exec_generated_code(v_code, globals())  # setzt valid_output, invalid_entries …\n",
    "            append_log(f\"[VALIDATOR] OK, valid_output={len(globals().get('valid_output', []))}\", DATA_PATH)\n",
    "        except Exception as exc:\n",
    "            snippet = error_snippet(exc)\n",
    "            print(\"⚠️  validator crashed – retrying …\\n\", snippet)\n",
    "            append_log(f\"[VALIDATOR] ERROR\\n{traceback.format_exc()}\", DATA_PATH)\n",
    "\n",
    "            for var in (\"output\", \"valid_output\", \"invalid_entries\", \"duplicate_count\"):\n",
    "                globals().pop(var, None)\n",
    "\n",
    "            retry_prompt = (build_transformation_prompt(ctx, meta)\n",
    "                            + f\"\\n[VALIDATION_ERROR]\\n{snippet}\")\n",
    "            save_artifact(retry_prompt, \"transform\", \"prompt\", DATA_PATH, attempt=1)\n",
    "            pth = save_artifact_copy(retry_prompt, \"transform\", \"prompt\", DATA_PATH, attempt=1, add_hash=False)\n",
    "            manifest_paths[\"transform\"][\"prompt\"].append(pth)\n",
    "\n",
    "            t_code = get_completion(retry_prompt)\n",
    "            save_artifact(extract_code_block(t_code), \"transform\", \"code\", DATA_PATH, attempt=1)\n",
    "            print(\"\\n— Retry Transformation Code —\\n\", t_code)\n",
    "            t_code_block = extract_code_block(t_code)\n",
    "            pth = save_artifact_copy(t_code_block, \"transform\", \"code\", DATA_PATH, attempt=1, add_hash=True)\n",
    "            manifest_paths[\"transform\"][\"code\"].append(pth)\n",
    "\n",
    "            exec_generated_code(t_code, globals())\n",
    "            if \"output\" not in globals():\n",
    "                raise RuntimeError(\"Retry produced no `output`\")\n",
    "\n",
    "            validator_prompt_retry = build_validator_prompt()\n",
    "            save_artifact(validator_prompt_retry, \"validator\", \"prompt\", DATA_PATH, attempt=1)\n",
    "            pth = save_artifact_copy(validator_prompt_retry, \"validator\", \"prompt\", DATA_PATH, attempt=1, add_hash=False)\n",
    "            manifest_paths[\"validator\"][\"prompt\"].append(pth)\n",
    "\n",
    "            v_code = get_completion(validator_prompt_retry)\n",
    "            save_artifact(extract_code_block(v_code), \"validator\", \"code\", DATA_PATH, attempt=1)\n",
    "            print(\"\\n— Retry Validator Code —\\n\", v_code)\n",
    "            v_code_block = extract_code_block(v_code)\n",
    "            pth = save_artifact_copy(v_code_block, \"validator\", \"code\", DATA_PATH, attempt=1, add_hash=True)\n",
    "            manifest_paths[\"validator\"][\"code\"].append(pth)\n",
    "\n",
    "            exec_generated_code(v_code, globals())\n",
    "            append_log(f\"[VALIDATOR][retry] OK, valid_output={len(globals().get('valid_output', []))}\", DATA_PATH)\n",
    "\n",
    "        # Validator-Invalids persistieren\n",
    "        val_invalid = globals().get(\"invalid_entries\", [])\n",
    "        val_invalid_count = 0\n",
    "        if val_invalid:\n",
    "            val_inv_path = INVALID_DIR / f\"{Path(DATA_PATH).stem}_invalid_validator.csv\"\n",
    "            pd.DataFrame(val_invalid).to_csv(val_inv_path, index=False)\n",
    "            val_invalid_count = len(val_invalid)\n",
    "            print(f\"❌  Validator flagged {val_invalid_count} rows  →  {val_inv_path}\")\n",
    "            append_log(f\"[VALIDATOR] invalid rows: {val_invalid_count} → {val_inv_path}\", DATA_PATH)\n",
    "\n",
    "        # Report + Persist final\n",
    "        completeness_report(df, output, valid_output, valid_output, val_invalid, CLEAN_DIR)\n",
    "        out_path = CLEAN_DIR / Path(DATA_PATH).name\n",
    "        pd.DataFrame(valid_output).to_csv(out_path, index=False)\n",
    "        print(f\"\\n✅ Bereinigter Datensatz gespeichert unter: {out_path}\")\n",
    "        append_log(f\"[OUTPUT] written → {out_path}\", DATA_PATH)\n",
    "\n",
    "        # Manifest\n",
    "        manifest = {\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"dataset\": str(DATA_PATH),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"sample_size\": SAMPLE_SIZE,\n",
    "            \"rng_state\": RNG_STATE,\n",
    "            \"outputs\": {\n",
    "                \"cleaned_csv\": str(out_path),\n",
    "                \"transform_invalid_count\": trans_invalid_count,\n",
    "                \"validator_invalid_count\": val_invalid_count\n",
    "            },\n",
    "            \"artifacts\": manifest_paths\n",
    "        }\n",
    "        man_path = write_manifest(DATA_PATH, manifest)\n",
    "        append_log(f\"[MANIFEST] {man_path}\", DATA_PATH)\n",
    "\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        append_log(f\"[FATAL] {traceback.format_exc()}\", DATA_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40804f",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d65be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1d/r43xvhb549d6dky2tk41bsjw0000gn/T/ipykernel_44175/2011407292.py:97: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(year_cluster)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Ungefilterter Wide-Frame gespeichert: merged/all_movies.csv\n",
      "— Fuzzy-Merge abgeschlossen (2025-08-31) —\n",
      "Eingelesene Quellen : 4\n",
      "Long-Records        : 39708\n",
      "Filme mit ≥2 Votes  : 9912\n",
      "Fuzzy-Duplikate     : 0\n",
      "👉 Gemergt  : merged/all_movies_wide_fuzzy.csv\n",
      "👉 Duplikate: merged/all_movies_fuzzy_duplicates.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re, textwrap, datetime as dt\n",
    "\n",
    "try:\n",
    "    from unidecode import unidecode   # Akzente → ASCII\n",
    "except ImportError:\n",
    "    unidecode = lambda s: s\n",
    "\n",
    "# Eingabe-/Ausgabe\n",
    "CLEAN_DIR = Path(\"cleaned\")\n",
    "MERGE_OUT = Path(\"merged/all_movies_wide_fuzzy.csv\")\n",
    "DUPL_OUT  = Path(\"merged/all_movies_fuzzy_duplicates.csv\")\n",
    "\n",
    "# Kanonische Rating-Spalten je Quelle\n",
    "CANON = {\n",
    "    \"imdb_data\":              \"rating_imdb\",\n",
    "    \"movielens_aggregated\":   \"rating_movielens\",\n",
    "    \"metacritic_movies\":      \"rating_metacritic\",\n",
    "    \"rotten_tomatoes_movies\": \"rating_rt_audience\",\n",
    "}\n",
    "\n",
    "# Kanonische ID-Spalten je Quelle\n",
    "ID_MAP = {\n",
    "    \"imdb_data\":              \"ID_IMDB\",\n",
    "    \"movielens_aggregated\":   \"ID_MOVIELENS\",\n",
    "    \"metacritic_movies\":      \"ID_METACRITIC\",\n",
    "    \"rotten_tomatoes_movies\": \"ID_RT\",\n",
    "}\n",
    "\n",
    "def standardize_id_column(df: pd.DataFrame, src: str) -> pd.DataFrame:\n",
    "    target = ID_MAP.get(src, f\"ID_{src.upper()}\")\n",
    "    if target in df.columns:\n",
    "        pass\n",
    "    elif \"ID\" in df.columns:\n",
    "        df = df.rename(columns={\"ID\": target})\n",
    "    else:\n",
    "        cands = [c for c in df.columns if str(c).upper().startswith(\"ID_\")]\n",
    "        if cands:\n",
    "            df = df.rename(columns={cands[0]: target})\n",
    "    if target in df.columns:\n",
    "        df[target] = pd.to_numeric(df[target], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "# 1) Clean-CSVs einsammeln\n",
    "frames = []\n",
    "for csv in CLEAN_DIR.glob(\"*.csv\"):\n",
    "    name = csv.name\n",
    "    if (\n",
    "        name.endswith((\"merged.csv\", \"all_movies_wide.csv\", \"all_movies_wide_fuzzy.csv\"))\n",
    "        or \"_invalid_records\" in name\n",
    "        or \"_duplicates\"      in name\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    src = csv.stem  # imdb_data, movielens_aggregated, …\n",
    "    df  = pd.read_csv(csv)\n",
    "\n",
    "    # Jahr vereinheitlichen\n",
    "    if \"year\" in df.columns and \"release_year\" not in df.columns:\n",
    "        df = df.rename(columns={\"year\": \"release_year\"})\n",
    "\n",
    "    # Rating in kanonische Spalte umbenennen (falls nötig)\n",
    "    rating_col = CANON.get(src, f\"rating_{src}\")\n",
    "    if \"rating\" in df.columns and rating_col not in df.columns:\n",
    "        df = df.rename(columns={\"rating\": rating_col})\n",
    "\n",
    "    # ID-Spalte standardisieren\n",
    "    df = standardize_id_column(df, src)\n",
    "\n",
    "    df[\"source\"] = src\n",
    "    frames.append(df)\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(f\"Keine geeigneten Clean-CSVs in {CLEAN_DIR}\")\n",
    "\n",
    "long_df = pd.concat(frames, ignore_index=True)\n",
    "# KEINE Titel-Normalisierung mehr:\n",
    "# long_df[\"norm_title\"] = long_df[\"title\"].map(norm_title)\n",
    "long_df[\"release_year\"] = pd.to_numeric(long_df[\"release_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 3) Jahr-Cluster (±1) – wie statisch\n",
    "def year_cluster(sub: pd.DataFrame) -> pd.Series:\n",
    "    years = sorted(set([y for y in sub[\"release_year\"].dropna()]))\n",
    "    clusters, cid = {}, 0\n",
    "    for y in years:\n",
    "        if any(abs(y - c) <= 1 for c in clusters.get(cid, [])):\n",
    "            clusters[cid].append(y)\n",
    "        else:\n",
    "            cid += 1; clusters[cid] = [y]\n",
    "    mapping = {y: c for c, ys in clusters.items() for y in ys}\n",
    "    return sub[\"release_year\"].map(mapping).fillna(cid + 1).astype(int)\n",
    "\n",
    "long_df[\"year_cluster\"] = (\n",
    "    long_df.groupby(\"title\", group_keys=False)\n",
    "           .apply(year_cluster)\n",
    ")\n",
    "\n",
    "# 4) Helper für gruppierte Aggregation\n",
    "def first_valid(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if not s.empty else np.nan\n",
    "\n",
    "def first_non_empty(series: pd.Series):\n",
    "    for val in series:\n",
    "        if isinstance(val, list) and val:\n",
    "            return val\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val\n",
    "    return np.nan\n",
    "\n",
    "group_cols = [\"title\", \"year_cluster\"]\n",
    "rating_cols = [c for c in long_df.columns if c.startswith(\"rating_\")]\n",
    "id_cols     = [c for c in long_df.columns if str(c).startswith(\"ID_\")]\n",
    "\n",
    "# 5) Ratings je Film (erste gültige)\n",
    "ratings_wide = (\n",
    "    long_df.groupby(group_cols, as_index=False)[rating_cols]\n",
    "           .agg(first_valid) if rating_cols else long_df[group_cols].drop_duplicates()\n",
    ")\n",
    "\n",
    "# IDs je Film (erste gültige)\n",
    "ids_map = (\n",
    "    long_df.groupby(group_cols, as_index=False)[id_cols]\n",
    "           .agg(first_valid) if id_cols else long_df[group_cols].drop_duplicates()\n",
    ")\n",
    "\n",
    "# 6) Repräsentativer Titel & kleinstes Jahr\n",
    "meta = (\n",
    "    long_df.sort_values([\"source\", \"title\"])\n",
    "           .groupby(group_cols, as_index=False)\n",
    "           .agg(title=(\"title\", \"first\"), release_year=(\"release_year\", \"min\"))\n",
    ")\n",
    "\n",
    "# 7) Genres: erste nicht-leere Liste\n",
    "genres_map = (\n",
    "    long_df.sort_values(\"source\")\n",
    "           .groupby(group_cols)[\"genres\"]\n",
    "           .apply(first_non_empty)\n",
    "           .reset_index(name=\"genres\")\n",
    ")\n",
    "\n",
    "# 8) Wide zusammenführen\n",
    "wide = ratings_wide.merge(meta,       on=group_cols, how=\"left\")\n",
    "wide = wide.merge(genres_map,         on=group_cols, how=\"left\")\n",
    "wide = wide.merge(ids_map,            on=group_cols, how=\"left\")\n",
    "\n",
    "# 9) Ungefiltertes Ergebnis speichern\n",
    "MERGE_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "UNFILTERED_OUT = Path(\"merged/all_movies.csv\")\n",
    "wide.to_csv(UNFILTERED_OUT, index=False)\n",
    "print(f\"💾 Ungefilterter Wide-Frame gespeichert: {UNFILTERED_OUT}\")\n",
    "\n",
    "# 10) ≥2 Ratings-Filter\n",
    "present_rating_mask = wide[rating_cols].notna().sum(axis=1) if rating_cols else pd.Series(0, index=wide.index)\n",
    "wide_filtered = wide.loc[present_rating_mask >= 2].copy()\n",
    "\n",
    "# 11) Finale Spalten (IDs nach vorne)\n",
    "ordered_cols = (id_cols + [\"title\", \"release_year\", \"genres\"] + rating_cols)\n",
    "ordered_cols = [c for c in ordered_cols if c in wide_filtered.columns]\n",
    "wide_filtered = wide_filtered[ordered_cols]\n",
    "\n",
    "# 12) Fuzzy-Duplikate (title, release_year)\n",
    "dup_mask   = wide_filtered.duplicated(subset=[\"title\", \"release_year\"], keep=False)\n",
    "duplicates = wide_filtered[dup_mask].copy()\n",
    "uniques    = wide_filtered[~dup_mask].copy()\n",
    "\n",
    "# 13) Endergebnisse speichern\n",
    "uniques.to_csv(MERGE_OUT, index=False)\n",
    "duplicates.to_csv(DUPL_OUT, index=False)\n",
    "\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "  — Fuzzy-Merge abgeschlossen ({dt.date.today()}) —\n",
    "  Eingelesene Quellen : {len(frames)}\n",
    "  Long-Records        : {len(long_df)}\n",
    "  Filme mit ≥2 Votes  : {len(uniques)}\n",
    "  Fuzzy-Duplikate     : {len(duplicates)}\n",
    "  👉 Gemergt  : {MERGE_OUT}\n",
    "  👉 Duplikate: {DUPL_OUT}\n",
    "\"\"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6075eaf",
   "metadata": {},
   "source": [
    "Superscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Superscore (0-10 Skala, ungewichtet – wie static_pipeline) ──────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap, datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "MERGED      = Path(\"merged/all_movies_wide_fuzzy.csv\")\n",
    "OUT_CSV     = Path(\"merged/all_movies_superscore_adaptive.csv\")\n",
    "MIN_RATINGS = 2\n",
    "\n",
    "df = pd.read_csv(MERGED)\n",
    "\n",
    "# ID-Spalten erkennen und sauber typisieren (bleiben bis zum Ende erhalten)\n",
    "id_cols = [c for c in df.columns if c == \"ID\" or str(c).startswith(\"ID_\")]\n",
    "for c in id_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Rating-Spalten finden\n",
    "rating_cols = [c for c in df.columns if c.startswith(\"rating_\")]\n",
    "if not rating_cols:\n",
    "    raise RuntimeError(\"Keine rating_*-Spalten gefunden.\")\n",
    "\n",
    "# 1. Quelle → 0-10-Normierung (heuristisch wie in static_pipeline)\n",
    "def to_0_10(series):\n",
    "    if series.dropna().empty:\n",
    "        return series\n",
    "    mx = series.max()\n",
    "    if mx <= 5.5:      # 0–5 Skala → *2 (MovieLens)\n",
    "        return series * 2\n",
    "    if mx > 10:        # 0–100 Skala → /10 (Metacritic, RT%)\n",
    "        return series / 10\n",
    "    return series      # 0–10 unverändert (IMDb)\n",
    "\n",
    "norm_cols = []\n",
    "for col in rating_cols:\n",
    "    ncol = col.replace(\"rating_\", \"\") + \"_norm\"\n",
    "    df[ncol] = to_0_10(df[col])\n",
    "    norm_cols.append(ncol)\n",
    "\n",
    "# 2. Anzahl verfügbarer normalisierter Ratings\n",
    "df[\"num_available_ratings\"] = df[norm_cols].notna().sum(axis=1)\n",
    "\n",
    "# 3. Superscore (nur wenn ≥ MIN_RATINGS normalisierte Ratings vorhanden)\n",
    "mask = df[\"num_available_ratings\"] >= MIN_RATINGS\n",
    "df.loc[mask, \"superscore_mean_0_10\"]   = df.loc[mask, norm_cols].mean(axis=1).round(1)\n",
    "df.loc[mask, \"superscore_median_0_10\"] = df.loc[mask, norm_cols].median(axis=1).round(1)\n",
    "\n",
    "# 4. Ausgabe – ID-Spalten vorne\n",
    "base_cols = [c for c in [\"title\", \"release_year\", \"genres\"] if c in df.columns]\n",
    "tail_cols = [\"num_available_ratings\", \"superscore_mean_0_10\", \"superscore_median_0_10\"]\n",
    "cols_out  = id_cols + base_cols + rating_cols + norm_cols + tail_cols\n",
    "cols_out  = [c for c in cols_out if c in df.columns]\n",
    "\n",
    "df[cols_out].to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "  — Superscore-Static abgeschlossen ({dt.date.today()}) —\n",
    "  Rating-Spalten erkannt : {rating_cols}\n",
    "  Normierte Spalten      : {norm_cols}\n",
    "  Filme mit ≥{MIN_RATINGS} Ratings : {int(mask.sum())} / {len(df)}\n",
    "  ID-Spalten im Output   : {id_cols}\n",
    "  👉 Ergebnis : {OUT_CSV}\n",
    "\"\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecfbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETL Pipeline (Linux)",
   "language": "python",
   "name": "etl-linux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
