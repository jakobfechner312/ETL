{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, textwrap, traceback\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import math, re\n",
    "\n",
    "# ───────── Config ─────────\n",
    "SAVE_DIR = Path(\"/Users/jakob/ba_etl/adaptive/cleaned\")     \n",
    "DATA_PATH   = \"/Users/jakob/ba_etl/adaptive/data/raw/rotten_tomatoes_movies.csv\" \n",
    "MODEL_NAME  = \"o4-mini\"\n",
    "SAMPLE_SIZE = 5\n",
    "RNG_STATE   = 42\n",
    "\n",
    "load_dotenv()                     # lädt Variablen aus .env\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ───────── Schema laden ─────────\n",
    "SCHEMA_PATH = Path(\"schema_testing.json\")\n",
    "SCHEMA_SPEC = json.loads(SCHEMA_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# ───────── Helper ─────────\n",
    "def schema_as_text(spec):\n",
    "    return \"\\n\".join(f\"- {d['field']}: (type: {d['type']}) {d['rule']}\"\n",
    "                     for d in spec)\n",
    "\n",
    "SCHEMA_TEXT = schema_as_text(SCHEMA_SPEC)\n",
    "\n",
    "def load_df(path=DATA_PATH):\n",
    "    return pd.read_csv(path, on_bad_lines=\"skip\")\n",
    "\n",
    "def make_context_and_meta(df):\n",
    "    ctx = df.sample(SAMPLE_SIZE, random_state=RNG_STATE).to_string(index=False)\n",
    "    buf = StringIO(); df.info(buf=buf)\n",
    "    return ctx, buf.getvalue()\n",
    "\n",
    "def get_completion(prompt: str):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "def exec_generated_code(code: str, g: dict):\n",
    "    \"\"\"Letzten ```-Block ausführen.\"\"\"\n",
    "    if \"```\" in code:\n",
    "        code = code.split(\"```\")[-2]\n",
    "    clean = code.replace(\"python\", \"\", 1).replace(\"```\", \"\").strip()\n",
    "    exec(clean, g)\n",
    "\n",
    "def error_snippet(exc, max_lines=10, max_chars=800):\n",
    "    tb = traceback.TracebackException.from_exception(exc, capture_locals=False)\n",
    "    txt = \"\".join(list(tb.format())[-max_lines:]).strip()\n",
    "    if isinstance(exc, AssertionError) and \"Invalid entries\" in str(exc):\n",
    "        txt = \"AssertionError: many invalid entries (gekürzt)\"\n",
    "    return (txt[:max_chars] + \" …\") if len(txt) > max_chars else txt\n",
    "\n",
    "def completeness_report(df0, out, v_llm, v_det, inv_det, savedir):\n",
    "    print(\"\\n— Completeness Report —\")\n",
    "    print(f\"Original rows:            {len(df0):>6}\")\n",
    "    print(f\"After transformation:     {len(out):>6}   (-{len(df0)-len(out)})\")\n",
    "    print(f\"LLM validator valid:      {len(v_llm):>6}   (-{len(out)-len(v_llm)})\")\n",
    "    if inv_det:\n",
    "        bad = pd.DataFrame(inv_det)\n",
    "        print(\"\\nBeispiele ungültiger Datensätze (dynamic):\")\n",
    "        print(bad.head().to_string(index=False))\n",
    "        rep = Path(savedir) / f\"{Path(DATA_PATH).stem}_invalid_records.csv\"\n",
    "        bad.to_csv(rep, index=False)\n",
    "        print(f\"\\n❌  Fehlerliste gespeichert unter: {rep}\")\n",
    "\n",
    "# ───────── Prompt-Builder ─────────\n",
    "def build_transformation_prompt(ctx, meta):\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    [ROLE]\n",
    "    You are an expert data scientist specialised in cleansing and standardising film datasets.\n",
    "\n",
    "    [PROCESS]\n",
    "      (a) [PLAN] Outline your high-level approach.\n",
    "      (b) [THINK] Write Pandas code step by step.\n",
    "      (c) [CHECK] Self-verify logic.\n",
    "      (d) [ANSWER] Return only executable Python starting with `output = []`\n",
    "\n",
    "    [SCHEMA]\n",
    "    {SCHEMA_TEXT}\n",
    "\n",
    "    [CONTEXT]\n",
    "    {ctx}\n",
    "\n",
    "    [METADATA]\n",
    "    {meta}\n",
    "    \"\"\").strip()\n",
    "\n",
    "def build_validator_prompt():\n",
    "    schema_json = json.dumps(SCHEMA_SPEC, indent=2, ensure_ascii=False)\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    [ROLE]\n",
    "    Validation stage of the ETL pipeline.\n",
    "\n",
    "    [SCHEMA_JSON]\n",
    "    {schema_json}\n",
    "\n",
    "    [PROCESS]\n",
    "      (a) [PLAN] outline checks\n",
    "      (b) [THINK] derive assertions\n",
    "      (c) [CHECK] ensure logic\n",
    "      (d) [ANSWER] one ```python``` block that\n",
    "          • iterates **over `output` (list of dicts)**        <─ WICHTIG\n",
    "          • builds valid_output / invalid_entries / duplicate_count\n",
    "          • raises AssertionError if invalid_entries is non-empty.\n",
    "\n",
    "    [FORMAT] Use [PLAN] [THINK] [CHECK] [ANSWER].\n",
    "    [OUTPUT] show only code inside the block.\n",
    "    \"\"\").strip()\n",
    "\n",
    "# ───────── Main ─────────\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Running with pandas\", pd.__version__)\n",
    "        df = load_df(); globals()[\"df\"] = df\n",
    "        ctx, meta = make_context_and_meta(df)\n",
    "\n",
    "        # 1) Transformation --------------------------------------------------\n",
    "        t_code = get_completion(build_transformation_prompt(ctx, meta))\n",
    "        print(\"\\n— Transformation Code —\\n\", t_code)\n",
    "        exec_generated_code(t_code, globals())\n",
    "        if \"output\" not in globals():\n",
    "            raise RuntimeError(\"LLM produced no `output`\")\n",
    "\n",
    "        # 2) LLM-Validator ---------------------------------------------------\n",
    "        v_code = get_completion(build_validator_prompt())\n",
    "        print(\"\\n— Validator Code —\\n\", v_code)\n",
    "        try:\n",
    "            exec_generated_code(v_code, globals())      # setzt valid_output\n",
    "        except Exception as exc:\n",
    "            # ─ Retry --------------------------------------------------------\n",
    "            snippet = error_snippet(exc)\n",
    "            print(\"⚠️  validator crashed – retrying …\\n\", snippet)\n",
    "            for var in (\"output\", \"valid_output\", \"invalid_entries\",\n",
    "                        \"duplicate_count\"):\n",
    "                globals().pop(var, None)\n",
    "\n",
    "            retry_prompt = (build_transformation_prompt(ctx, meta)\n",
    "                            + f\"\\n[VALIDATION_ERROR]\\n{snippet}\")\n",
    "            t_code = get_completion(retry_prompt)\n",
    "            print(\"\\n— Retry Transformation Code —\\n\", t_code)\n",
    "            exec_generated_code(t_code, globals())\n",
    "            if \"output\" not in globals():\n",
    "                raise RuntimeError(\"Retry produced no `output`\")\n",
    "\n",
    "            v_code = get_completion(build_validator_prompt())\n",
    "            print(\"\\n— Retry Validator Code —\\n\", v_code)\n",
    "            exec_generated_code(v_code, globals())      # bricht hier, wenn wieder Fehler\n",
    "        # 3) Report + Persist  -------------------------------------------------\n",
    "        completeness_report(df, output, valid_output,\n",
    "                            valid_output, [],          # Platzhalter\n",
    "                            \"cleaned\")\n",
    "        \n",
    "        SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = SAVE_DIR / Path(DATA_PATH).name\n",
    "        pd.DataFrame(valid_output).to_csv(out_path, index=False)\n",
    "        print(f\"\\n✅ Bereinigter Datensatz gespeichert unter: {out_path}\")\n",
    "\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Fuzzy-Merge aller Clean-CSVs (Titel-Normalisierung + ±1-Jahr) ──────\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re, textwrap, datetime as dt\n",
    "\n",
    "try:\n",
    "    from unidecode import unidecode          # Akzente ➜ ASCII\n",
    "except ImportError:\n",
    "    unidecode = lambda s: s                  # Fallback, falls Paket fehlt\n",
    "\n",
    "# Basisordner: hier liegen die bereinigten Einzel-CSVs\n",
    "CLEAN_DIR  = Path(\"cleaned\")\n",
    "\n",
    "# Zielpfade\n",
    "MERGE_OUT  = Path(\"merged/all_movies_wide_fuzzy.csv\")\n",
    "DUPL_OUT   = Path(\"merged/all_movies_fuzzy_duplicates.csv\")\n",
    "\n",
    "# 0. Kanonisches Spaltenmapping  ---------------------------------------------\n",
    "CANON = {\n",
    "    \"imdb_data\":              \"rating_imdb\",\n",
    "    \"movielens_aggregated\":   \"rating_movielens\",\n",
    "    \"metacritic_movies\":      \"rating_metacritic\",\n",
    "    \"rotten_tomatoes_movies\": \"rating_rt_audience\",\n",
    "}\n",
    "\n",
    "# 1. Clean-CSVs einsammeln  ---------------------------------------------------\n",
    "frames = []\n",
    "for csv in CLEAN_DIR.glob(\"*.csv\"):\n",
    "    name = csv.name\n",
    "    if (\n",
    "        name.endswith((\"merged.csv\", \"all_movies_wide.csv\", \"all_movies_wide_fuzzy.csv\"))\n",
    "        or \"_invalid_records\" in name\n",
    "        or \"_duplicates\"      in name\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    src = csv.stem                      # imdb_data, movielens_aggregated, …\n",
    "    df  = pd.read_csv(csv)\n",
    "\n",
    "    # Jahr-Spalte vereinheitlichen\n",
    "    if \"year\" in df.columns and \"release_year\" not in df.columns:\n",
    "        df = df.rename(columns={\"year\": \"release_year\"})\n",
    "\n",
    "    # Rating-Spalte vereinheitlichen\n",
    "    rating_col = CANON.get(src, f\"rating_{src}\")      # Fallback wie vorher\n",
    "    df = df.rename(columns={\"rating\": rating_col})\n",
    "\n",
    "    df[\"source\"] = src\n",
    "    frames.append(df)\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(f\"Keine geeigneten Clean-CSVs in {CLEAN_DIR}\")\n",
    "\n",
    "long_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# 2. Titel aggressiv normalisieren  ------------------------------------------\n",
    "def norm_title(title: str) -> str:\n",
    "    if not isinstance(title, str):\n",
    "        return \"\"\n",
    "    t = unidecode(title).lower()\n",
    "    t = re.sub(r\"\\s*\\(\\d{4}\\)$\", \"\", t)          # (YYYY) am Ende entfernen\n",
    "    t = re.sub(r\"\\s*\\([^)]*\\)$\", \"\", t)          # beliebige Alias-Klammer am Ende\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)               # Satzzeichen → Leerzeichen\n",
    "    t = re.sub(r\"\\bthe\\s*$\", \"\", t)              # trailing \"the\"\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # doppelte Tokens entfernen\n",
    "    seen, tokens = set(), []\n",
    "    for tok in t.split():\n",
    "        if tok not in seen:\n",
    "            tokens.append(tok); seen.add(tok)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "long_df[\"norm_title\"]   = long_df[\"title\"].map(norm_title)\n",
    "long_df[\"release_year\"] = pd.to_numeric(long_df[\"release_year\"],\n",
    "                                        errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 3. Jahr-Cluster innerhalb ±1  ----------------------------------------------\n",
    "def year_cluster(sub):\n",
    "    years = sorted(set([y for y in sub[\"release_year\"].dropna()]))\n",
    "    clusters, cid = {}, 0\n",
    "    for y in years:\n",
    "        if any(abs(y - c) <= 1 for c in clusters.get(cid, [])):\n",
    "            clusters[cid].append(y)\n",
    "        else:\n",
    "            cid += 1; clusters[cid] = [y]\n",
    "    mapping = {y: c for c, ys in clusters.items() for y in ys}\n",
    "    return sub[\"release_year\"].map(mapping).fillna(cid + 1).astype(int)\n",
    "\n",
    "long_df[\"year_cluster\"] = (\n",
    "    long_df.groupby(\"norm_title\", group_keys=False)\n",
    "           .apply(year_cluster)\n",
    ")\n",
    "\n",
    "# 4. Genres-Priorität (erste nicht-leere Liste) -------------------------------\n",
    "genres_map = (\n",
    "    long_df\n",
    "      .sort_values(\"source\")\n",
    "      .groupby([\"norm_title\", \"year_cluster\"])[\"genres\"]\n",
    "      .first()\n",
    "      .rename(\"genres_any\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# 5. Pivot → Wide  ------------------------------------------------------------\n",
    "rating_cols = [c for c in long_df.columns if c.startswith(\"rating_\")]\n",
    "wide = (\n",
    "    long_df\n",
    "      .pivot_table(index=[\"norm_title\", \"year_cluster\"],\n",
    "                   columns=\"source\",\n",
    "                   values=rating_cols,\n",
    "                   aggfunc=\"first\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Spaltenindex flatten\n",
    "wide.columns = [\n",
    "    col[0] if isinstance(col, tuple) else col\n",
    "    for col in wide.columns\n",
    "]\n",
    "\n",
    "# 6. Repräsentativen Titel & kleinstes Jahr wählen ----------------------------\n",
    "meta = (\n",
    "    long_df\n",
    "      .sort_values([\"source\", \"title\"])\n",
    "      .groupby([\"norm_title\", \"year_cluster\"])\n",
    "      .agg({\"title\": \"first\", \"release_year\": \"min\"})\n",
    "      .reset_index()\n",
    ")\n",
    "wide = wide.merge(meta, on=[\"norm_title\", \"year_cluster\"], how=\"left\")\n",
    "\n",
    "# 7. Genres mergen  -----------------------------------------------------------\n",
    "wide = wide.merge(genres_map, on=[\"norm_title\", \"year_cluster\"], how=\"left\")\n",
    "\n",
    "# --- Zwischenergebnis VOR dem ≥2-Ratings-Filter speichern ---------------\n",
    "UNFILTERED_OUT = Path(\"merged/all_movies.csv\")   # bewusst anderer Name\n",
    "wide.to_csv(UNFILTERED_OUT, index=False)\n",
    "print(f\"💾 Ungefilterter Wide-Frame gespeichert: {UNFILTERED_OUT}\")\n",
    "\n",
    "# 8. Nur Filme mit ≥ 2 vorhandenen Ratings behalten ---------------------------\n",
    "rating_cols = [c for c in wide.columns if c.startswith(\"rating_\")]\n",
    "wide = wide[wide[rating_cols].notna().sum(axis=1) >= 2]\n",
    "\n",
    "# 9. Spalten sortieren & aufräumen -------------------------------------------\n",
    "cols_order = [\"title\", \"release_year\", \"genres_any\"] + rating_cols\n",
    "wide = wide[cols_order].rename(columns={\"genres_any\": \"genres\"})\n",
    "\n",
    "# 10. Fuzzy-Duplikate (falls dennoch Gleiches übrig) --------------------------\n",
    "dup_mask  = wide.duplicated(subset=[\"title\", \"release_year\"], keep=False)\n",
    "duplicates = wide[dup_mask].copy()\n",
    "uniques    = wide[~dup_mask].copy()\n",
    "\n",
    "# 11. Speichern  --------------------------------------------------------------\n",
    "MERGE_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "uniques.to_csv(MERGE_OUT, index=False)\n",
    "duplicates.to_csv(DUPL_OUT, index=False)\n",
    "\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "  — Fuzzy-Merge abgeschlossen ({dt.date.today()}) —\n",
    "  Eingelesene Quellen : {len(frames)}\n",
    "  Long-Records        : {len(long_df)}\n",
    "  Filme mit ≥2 Votes  : {len(uniques)}\n",
    "  Fuzzy-Duplikate     : {len(duplicates)}\n",
    "  👉 Gemergt  : {MERGE_OUT}\n",
    "  👉 Duplikate: {DUPL_OUT}\n",
    "\"\"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Superscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Superscore (0-10 Skala, ungewichtet – wie static_pipeline) ──────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap, datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "MERGED      = Path(\"testing/cleaned/merged/all_movies_wide_fuzzy.csv\")\n",
    "OUT_CSV     = Path(\"testing/cleaned/merged/all_movies_superscore_static.csv\")\n",
    "MIN_RATINGS = 2\n",
    "\n",
    "df = pd.read_csv(MERGED)\n",
    "rating_cols = [c for c in df.columns if c.startswith(\"rating_\")]\n",
    "if not rating_cols:\n",
    "    raise RuntimeError(\"Keine rating_*-Spalten gefunden.\")\n",
    "\n",
    "# 1. Quelle → 0-10-Normierung  (heuristisch wie in static_pipeline)\n",
    "def to_0_10(series):\n",
    "    if series.dropna().empty:\n",
    "        return series\n",
    "    mx = series.max()\n",
    "    # 0-5 → *2  (MovieLens)\n",
    "    if mx <= 5.5:\n",
    "        return series * 2\n",
    "    # 0-100 → /10 (Metacritic, RT%)\n",
    "    if mx > 10:\n",
    "        return series / 10\n",
    "    # sonst 0-10 unverändert (IMDb & Co)\n",
    "    return series\n",
    "\n",
    "norm_cols = []\n",
    "for col in rating_cols:\n",
    "    ncol = col.replace(\"rating_\", \"\") + \"_norm\"   # imdb_norm, metacritic_norm …\n",
    "    df[ncol] = to_0_10(df[col])\n",
    "    norm_cols.append(ncol)\n",
    "\n",
    "# 2. Anzahl verfügbarer normierte Ratings\n",
    "df[\"num_available_ratings\"] = df[norm_cols].notna().sum(axis=1)\n",
    "\n",
    "# 3. Nur Filme mit ≥ MIN_RATINGS in Berechnung einbeziehen\n",
    "mask = df[\"num_available_ratings\"] >= MIN_RATINGS\n",
    "df.loc[mask, \"superscore_mean_0_10\"]   = df.loc[mask, norm_cols].mean(axis=1).round(1)\n",
    "df.loc[mask, \"superscore_median_0_10\"] = df.loc[mask, norm_cols].median(axis=1).round(1)\n",
    "\n",
    "# 4. Speichern\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "  — Superscore-Static abgeschlossen ({dt.date.today()}) —\n",
    "  Rating-Spalten erkannt : {rating_cols}\n",
    "  Normierte Spalten      : {norm_cols}\n",
    "  Filme mit ≥{MIN_RATINGS} Ratings : {mask.sum()} / {len(df)}\n",
    "  👉 Ergebnis : {OUT_CSV}\n",
    "\"\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
