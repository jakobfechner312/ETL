{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53041b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, textwrap, traceback\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "from unidecode import unidecode\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import math, re\n",
    "\n",
    "# === ADDED ===\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "# === /ADDED ===\n",
    "\n",
    "# ───────── Config ─────────\n",
    "SAVE_DIR = Path(\"/Users/jakob/ba_etl/adaptive/cleaned\")     \n",
    "DATA_PATH   = \"/Users/jakob/ba_etl/adaptive/data/raw/imdb_data.csv\"\n",
    "MODEL_NAME  = \"o4-mini\"\n",
    "SAMPLE_SIZE = 5\n",
    "RNG_STATE   = 42\n",
    "\n",
    "load_dotenv()                     \n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ───────── Artefakt-Ordner und Helper ─────────\n",
    "ARTIFACT_DIR = Path(\"/Users/jakob/ba_etl/adaptive/run_artifacts\")\n",
    "\n",
    "# === ADDED ===\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "def run_dir_for(dataset_path: str) -> Path:\n",
    "    \"\"\"Pro Run eigener Artefaktordner: <stem>_<RUN_ID>\"\"\"\n",
    "    stem = Path(dataset_path).stem\n",
    "    d = ARTIFACT_DIR / f\"{stem}_{RUN_ID}\"\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def append_log(msg: str, dataset_path: str, fname: str = \"run.log\"):\n",
    "    rd = run_dir_for(dataset_path)\n",
    "    with open(rd / fname, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(msg.rstrip() + \"\\n\")\n",
    "\n",
    "def save_artifact_copy(text: str, stage: str, kind: str,\n",
    "                       dataset_path: str, attempt: int = 0, add_hash: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Legt eine *zusätzliche* Kopie im Run-Ordner ab (ohne das bestehende save_artifact zu verändern).\n",
    "    stage : 'transform' | 'validator'\n",
    "    kind  : 'prompt'    | 'code'\n",
    "    \"\"\"\n",
    "    run_dir = run_dir_for(dataset_path)\n",
    "    suffix = \"_retry\" if attempt else \"\"\n",
    "    ext = \"txt\" if kind == \"prompt\" else \"py\"\n",
    "    fname = f\"{stage}_{kind}{suffix}\"\n",
    "    if add_hash:\n",
    "        digest = hashlib.sha256(text.encode(\"utf-8\")).hexdigest()[:12]\n",
    "        fname += f\"_{digest}\"\n",
    "    target = run_dir / f\"{fname}.{ext}\"\n",
    "    target.write_text(text, encoding=\"utf-8\")\n",
    "    return str(target)\n",
    "\n",
    "def write_manifest(dataset_path: str, meta: dict) -> str:\n",
    "    rd = run_dir_for(dataset_path)\n",
    "    p = rd / \"manifest.json\"\n",
    "    p.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "    return str(p)\n",
    "# === /ADDED ===\n",
    "\n",
    "# ───────── Schema laden ─────────\n",
    "SCHEMA_PATH = Path(\"schema.json\")\n",
    "SCHEMA_SPEC = json.loads(SCHEMA_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# ───────── Helper ─────────\n",
    "def schema_as_text(spec):\n",
    "    return \"\\n\".join(f\"- {d['field']}: (type: {d['type']}) {d['rule']}\"\n",
    "                     for d in spec)\n",
    "\n",
    "def infer_dataset_tag(path: str) -> str:\n",
    "    name = Path(path).name.lower()\n",
    "    if \"imdb\" in name: return \"IMDB\"\n",
    "    if \"movielens\" in name: return \"MOVIELENS\"\n",
    "    if \"metacritic\" in name: return \"METACRITIC\"\n",
    "    if \"rotten\" in name or \"rt\" in name: return \"RT\"\n",
    "    return \"DATA\"\n",
    "\n",
    "DATASET_TAG = infer_dataset_tag(DATA_PATH)\n",
    "ID_COL = f\"ID_{DATASET_TAG}\"\n",
    "\n",
    "SCHEMA_TEXT = schema_as_text(SCHEMA_SPEC)\n",
    "\n",
    "def load_df(path=DATA_PATH):\n",
    "    return pd.read_csv(path, on_bad_lines=\"skip\")\n",
    "\n",
    "def make_context_and_meta(df):\n",
    "    ctx = df.sample(SAMPLE_SIZE, random_state=RNG_STATE).to_string(index=False)\n",
    "    buf = StringIO(); df.info(buf=buf)\n",
    "    return ctx, buf.getvalue()\n",
    "\n",
    "def get_completion(prompt: str):\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    ).choices[0].message.content\n",
    "\n",
    "def exec_generated_code(code: str, g: dict):\n",
    "    \"\"\"Letzten ```-Block ausführen.\"\"\"\n",
    "    if \"```\" in code:\n",
    "        code = code.split(\"```\")[-2]\n",
    "    clean = code.replace(\"python\", \"\", 1).replace(\"```\", \"\").strip()\n",
    "    exec(clean, g)\n",
    "\n",
    "def error_snippet(exc, max_lines=20, max_chars=1500):\n",
    "    tb = traceback.TracebackException.from_exception(exc, capture_locals=False)\n",
    "    frames = list(tb.format())\n",
    "    txt = \"\".join(frames[:5] + frames[-max_lines:]).strip()\n",
    "\n",
    "    # AssertionError: füge Beispiele an\n",
    "    if isinstance(exc, AssertionError) and \"invalid_entries\" in str(exc):\n",
    "        msg = str(exc)\n",
    "        truncated = msg[:500] + (\" …\" if len(msg) > 500 else \"\")\n",
    "        txt = truncated + \"\\n\\n\" + txt\n",
    "\n",
    "    return (txt[:max_chars] + \" …\") if len(txt) > max_chars else txt\n",
    "\n",
    "def extract_code_block(md: str) -> str:\n",
    "    \"\"\"Extrahiert den letzten ```-Block und entfernt Markdown-Header.\"\"\"\n",
    "    if \"```\" in md:\n",
    "        md = md.split(\"```\")[-2]\n",
    "    return md.replace(\"python\", \"\", 1).replace(\"```\", \"\").strip()\n",
    "\n",
    "def save_artifact(text: str, stage: str, kind: str,\n",
    "                  dataset_path: str, attempt: int = 0):\n",
    "    \"\"\"\n",
    "    stage  : 'transform' | 'validator'\n",
    "    kind   : 'prompt'    | 'code'\n",
    "    attempt: 0 = erster Versuch, 1 = Retry\n",
    "    \"\"\"\n",
    "    stem = Path(dataset_path).stem\n",
    "    suffix = \"_retry\" if attempt else \"\"\n",
    "    ext = \"txt\" if kind == \"prompt\" else \"py\"\n",
    "    target_dir = ARTIFACT_DIR / stem\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (target_dir / f\"{stage}_{kind}{suffix}.{ext}\").write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "def completeness_report(df0, out, v_llm, v_det, inv_det, savedir):\n",
    "    print(\"\\n— Completeness Report —\")\n",
    "    print(f\"Original rows:            {len(df0):>6}\")\n",
    "    print(f\"After transformation:     {len(out):>6}   (-{len(df0)-len(out)})\")\n",
    "    print(f\"LLM validator valid:      {len(v_llm):>6}   (-{len(out)-len(v_llm)})\")\n",
    "    if inv_det:\n",
    "        bad = pd.DataFrame(inv_det)\n",
    "        print(\"\\nBeispiele ungültiger Datensätze (dynamic):\")\n",
    "        print(bad.head().to_string(index=False))\n",
    "        rep = Path(savedir) / f\"{Path(DATA_PATH).stem}_invalid_records.csv\"\n",
    "        bad.to_csv(rep, index=False)\n",
    "        print(f\"\\n❌  Fehlerliste gespeichert unter: {rep}\")\n",
    "    # === ADDED: Report zusätzlich im Run-Ordner ablegen ===\n",
    "    lines = [\n",
    "        \"— Completeness Report —\",\n",
    "        f\"Original rows:            {len(df0):>6}\",\n",
    "        f\"After transformation:     {len(out):>6}   (-{len(df0)-len(out)})\",\n",
    "        f\"LLM validator valid:      {len(v_llm):>6}   (-{len(out)-len(v_llm)})\",\n",
    "    ]\n",
    "    (run_dir_for(DATA_PATH) / \"completeness.txt\").write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    # === /ADDED ===\n",
    "\n",
    "# ───────── Prompt-Builder ─────────\n",
    "def build_transformation_prompt(ctx, meta):\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    [ROLE]\n",
    "    You are an expert data scientist specialised in cleansing and standardising film datasets.\n",
    "    Try to interpret the schema and the context and the metadata to understand the data.\n",
    "\n",
    "    [PROCESS]\n",
    "      (a) [PLAN] Outline your high-level approach.\n",
    "      (b) [THINK] Write Pandas code step by step to fullfill the {SCHEMA_TEXT}.\n",
    "      (c) [CHECK] Self-verify logic.\n",
    "      (d) [ANSWER] return one ```python``` block that:\n",
    "          • starts with  output = []\n",
    "          • Select the most complete numeric rating, prefer original ratings, fallback if empty.\n",
    "          • preserve the column {ID_COL} unchanged in every output row as the ID column\n",
    "          • interprets two-digit years so that the final four-digit year\n",
    "            falls in the realistic range 1900-2025\n",
    "          • genres: list of strings \n",
    "          • title: normalize exactly: ascii+lower; remove one trailing \"(YYYY)\" then one trailing \"(...)\"; punctuation→space; collapse spaces+trim; drop trailing \"the\"; dedup tokens (keep order).\n",
    "          • release_year: could also be \"streaming_release_year\", if necessary extract it \n",
    "          • interprets two-digit years so that the final four-digit year\n",
    "            falls in the realistic range 1900-2025\n",
    "          • builds an additional list  invalid_entries\n",
    "            – append a dict whenever a row is skipped\n",
    "              {{ \"row\": <original_row_as_dict>, \"reason\": \"<short text>\" }}\n",
    "          \n",
    "                           \n",
    "    [SCHEMA]\n",
    "    {SCHEMA_TEXT}\n",
    "\n",
    "    [CONTEXT]\n",
    "    {ctx}\n",
    "\n",
    "    [METADATA]\n",
    "    {meta}\n",
    "    \"\"\").strip()\n",
    "\n",
    "def build_validator_prompt():\n",
    "    schema_json = json.dumps(SCHEMA_SPEC, indent=2, ensure_ascii=False)\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    [ROLE]\n",
    "    Validation stage of the ETL pipeline.\n",
    "\n",
    "    [SCHEMA_JSON]\n",
    "    {schema_json}\n",
    "\n",
    "    [PROCESS]\n",
    "      (a) [PLAN] outline checks\n",
    "      (b) [THINK] derive assertions\n",
    "      (c) [CHECK] ensure logic\n",
    "      (d) [ANSWER] one ```python``` block that\n",
    "          • iterates **over `output` (list of dicts)**\n",
    "          • builds valid_output / invalid_entries / duplicate_count\n",
    "          • raises AssertionError if invalid_entries is non-empty.\n",
    "\n",
    "    [FORMAT] Use [PLAN] [THINK] [CHECK] [ANSWER].\n",
    "    [OUTPUT] show only code inside the block.\n",
    "    \"\"\").strip()\n",
    "\n",
    "# ───────── Main ─────────\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Running with pandas\", pd.__version__)\n",
    "        append_log(f\"RUN {RUN_ID} started with pandas {pd.__version__}\", DATA_PATH)\n",
    "\n",
    "        df = load_df(); globals()[\"df\"] = df\n",
    "        df[ID_COL] = range(1, len(df)+1)   # 1..N pro geladener Datei\n",
    "        globals()[\"ID_COL\"] = ID_COL  \n",
    "        ctx, meta = make_context_and_meta(df)\n",
    "\n",
    "        # 1) Transformation --------------------------------------------------\n",
    "        snippet = \"\"\n",
    "        # ===  Artefaktpfade für Manifest sammeln ===\n",
    "        manifest_paths = {\n",
    "            \"transform\": {\"prompt\": [], \"code\": []},\n",
    "            \"validator\": {\"prompt\": [], \"code\": []},\n",
    "        }\n",
    "        # === /ADDED ===\n",
    "\n",
    "        for attempt in range(2):          # 0 = erster Versuch, 1 = Retry\n",
    "            try:\n",
    "                prompt = build_transformation_prompt(ctx, meta)\n",
    "                if attempt == 1:          # beim Retry Fehlersnippet anhängen\n",
    "                    prompt += f\"\\n[TRANSFORMATION_ERROR]\\n{snippet}\"\n",
    "                    \n",
    "                save_artifact(prompt, \"transform\", \"prompt\", DATA_PATH, attempt) \n",
    "                # === zusätzliche Kopie im Run-Ordner ===\n",
    "                pth = save_artifact_copy(prompt, \"transform\", \"prompt\", DATA_PATH, attempt, add_hash=False)\n",
    "                manifest_paths[\"transform\"][\"prompt\"].append(pth)\n",
    "                t_code = get_completion(prompt)\n",
    "                # Bestehende Speicherung beibehalten:\n",
    "                save_artifact(\n",
    "                    extract_code_block(t_code),\n",
    "                    \"transform\", \"code\", DATA_PATH, attempt)\n",
    "                print(\"\\n— Transformation Code —\\n\", t_code)\n",
    "                # ===  Hash-Kopie ablegen ===\n",
    "                t_code_block = extract_code_block(t_code)\n",
    "                pth = save_artifact_copy(t_code_block, \"transform\", \"code\", DATA_PATH, attempt, add_hash=True)\n",
    "                manifest_paths[\"transform\"][\"code\"].append(pth)\n",
    "\n",
    "                exec_generated_code(t_code, globals())\n",
    "                if \"output\" not in globals():\n",
    "                    raise RuntimeError(\"LLM produced no `output`\")\n",
    "\n",
    "                append_log(f\"[TRANSFORM][attempt={attempt}] OK, rows={len(globals()['output'])}\", DATA_PATH)\n",
    "                break                     # erfolgreich → Schleife beenden\n",
    "\n",
    "            except Exception as exc:\n",
    "                snippet = error_snippet(exc)\n",
    "                print(\"⚠️  transformation crashed – retrying …\\n\", snippet)\n",
    "                append_log(f\"[TRANSFORM][attempt={attempt}] ERROR\\n{traceback.format_exc()}\", DATA_PATH)\n",
    "                globals().pop(\"output\", None)     # aufräumen\n",
    "                if attempt == 1:                  # zweiter Fehlschlag → endgültig\n",
    "                    raise\n",
    "        \n",
    "        # -------- persist rows dropped by the transformation ----------------\n",
    "        trans_invalid = globals().get(\"invalid_entries\", [])\n",
    "        trans_invalid_count = 0\n",
    "        if trans_invalid:\n",
    "            inv_df = pd.DataFrame(trans_invalid)\n",
    "            SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            tr_inv_path = SAVE_DIR / f\"{Path(DATA_PATH).stem}_invalid.csv\"\n",
    "            inv_df.to_csv(tr_inv_path, index=False)\n",
    "            trans_invalid_count = len(inv_df)\n",
    "            print(f\"\\n❌  Transform dropped {trans_invalid_count} rows  →  {tr_inv_path}\")\n",
    "            append_log(f\"[TRANSFORM] dropped {trans_invalid_count} rows → {tr_inv_path}\", DATA_PATH)\n",
    "        # Liste leeren, damit der Validator seine eigene invalid_entries anlegt\n",
    "        globals().pop(\"invalid_entries\", None)\n",
    "    \n",
    "        # 2) LLM-Validator ---------------------------------------------------\n",
    "        validator_prompt = build_validator_prompt()\n",
    "        save_artifact(validator_prompt, \"validator\", \"prompt\", DATA_PATH)          # Prompt sichern\n",
    "        # === ADDED ===\n",
    "        pth = save_artifact_copy(validator_prompt, \"validator\", \"prompt\", DATA_PATH, attempt=0, add_hash=False)\n",
    "        manifest_paths[\"validator\"][\"prompt\"].append(pth)\n",
    "        # === /ADDED ===\n",
    "\n",
    "        v_code = get_completion(validator_prompt)\n",
    "        save_artifact(extract_code_block(v_code), \"validator\", \"code\", DATA_PATH)  # Code sichern\n",
    "        print(\"\\n— Validator Code —\\n\", v_code)\n",
    "        # === ADDED: Hash-Kopie ablegen ===\n",
    "        v_code_block = extract_code_block(v_code)\n",
    "        pth = save_artifact_copy(v_code_block, \"validator\", \"code\", DATA_PATH, attempt=0, add_hash=True)\n",
    "        manifest_paths[\"validator\"][\"code\"].append(pth)\n",
    "        # === /ADDED ===\n",
    "\n",
    "        try:\n",
    "            exec_generated_code(v_code, globals())      # setzt valid_output, invalid_entries …\n",
    "            append_log(f\"[VALIDATOR] OK, valid_output={len(globals().get('valid_output', []))}\", DATA_PATH)\n",
    "        except Exception as exc:\n",
    "            # ─ Retry --------------------------------------------------------\n",
    "            snippet = error_snippet(exc)\n",
    "            print(\"⚠️  validator crashed – retrying …\\n\", snippet)\n",
    "            append_log(f\"[VALIDATOR] ERROR\\n{traceback.format_exc()}\", DATA_PATH)\n",
    "\n",
    "            for var in (\"output\", \"valid_output\", \"invalid_entries\", \"duplicate_count\"):\n",
    "                globals().pop(var, None)\n",
    "\n",
    "            retry_prompt = (build_transformation_prompt(ctx, meta)\n",
    "                            + f\"\\n[VALIDATION_ERROR]\\n{snippet}\")\n",
    "            \n",
    "            save_artifact(retry_prompt, \"transform\", \"prompt\", DATA_PATH, attempt=1)\n",
    "            pth = save_artifact_copy(retry_prompt, \"transform\", \"prompt\", DATA_PATH, attempt=1, add_hash=False)\n",
    "            manifest_paths[\"transform\"][\"prompt\"].append(pth)\n",
    "\n",
    "            t_code = get_completion(retry_prompt)\n",
    "            save_artifact(extract_code_block(t_code), \"transform\", \"code\", DATA_PATH, attempt=1)\n",
    "            print(\"\\n— Retry Transformation Code —\\n\", t_code)\n",
    "            t_code_block = extract_code_block(t_code)\n",
    "            pth = save_artifact_copy(t_code_block, \"transform\", \"code\", DATA_PATH, attempt=1, add_hash=True)\n",
    "            manifest_paths[\"transform\"][\"code\"].append(pth)\n",
    "\n",
    "            exec_generated_code(t_code, globals())\n",
    "            if \"output\" not in globals():\n",
    "                raise RuntimeError(\"Retry produced no `output`\")\n",
    "\n",
    "            validator_prompt_retry = build_validator_prompt()\n",
    "            save_artifact(validator_prompt_retry, \"validator\", \"prompt\", DATA_PATH, attempt=1)\n",
    "            pth = save_artifact_copy(validator_prompt_retry, \"validator\", \"prompt\", DATA_PATH, attempt=1, add_hash=False)\n",
    "            manifest_paths[\"validator\"][\"prompt\"].append(pth)\n",
    "\n",
    "            v_code = get_completion(validator_prompt_retry)\n",
    "            save_artifact(extract_code_block(v_code), \"validator\", \"code\", DATA_PATH, attempt=1)\n",
    "            print(\"\\n— Retry Validator Code —\\n\", v_code)\n",
    "            v_code_block = extract_code_block(v_code)\n",
    "            pth = save_artifact_copy(v_code_block, \"validator\", \"code\", DATA_PATH, attempt=1, add_hash=True)\n",
    "            manifest_paths[\"validator\"][\"code\"].append(pth)\n",
    "\n",
    "            exec_generated_code(v_code, globals())\n",
    "            append_log(f\"[VALIDATOR][retry] OK, valid_output={len(globals().get('valid_output', []))}\", DATA_PATH)\n",
    "\n",
    "        # === ADDED: Validator-Fehlerliste persistieren ===\n",
    "        val_invalid = globals().get(\"invalid_entries\", [])\n",
    "        val_invalid_count = 0\n",
    "        if val_invalid:\n",
    "            SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            val_inv_path = SAVE_DIR / f\"{Path(DATA_PATH).stem}_invalid_validator.csv\"\n",
    "            pd.DataFrame(val_invalid).to_csv(val_inv_path, index=False)\n",
    "            val_invalid_count = len(val_invalid)\n",
    "            print(f\"❌  Validator flagged {val_invalid_count} rows  →  {val_inv_path}\")\n",
    "            append_log(f\"[VALIDATOR] invalid rows: {val_invalid_count} → {val_inv_path}\", DATA_PATH)\n",
    "        # 3) Report + Persist  -------------------------------------------------\n",
    "        completeness_report(df, output, valid_output,\n",
    "                            valid_output, val_invalid,     # <= hier nicht mehr []\n",
    "                            \"cleaned\")\n",
    "        \n",
    "        SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        out_path = SAVE_DIR / Path(DATA_PATH).name\n",
    "        pd.DataFrame(valid_output).to_csv(out_path, index=False)\n",
    "        print(f\"\\n✅ Bereinigter Datensatz gespeichert unter: {out_path}\")\n",
    "        append_log(f\"[OUTPUT] written → {out_path}\", DATA_PATH)\n",
    "\n",
    "        # === ADDED: Manifest schreiben ===\n",
    "        manifest = {\n",
    "            \"run_id\": RUN_ID,\n",
    "            \"dataset\": str(DATA_PATH),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"sample_size\": SAMPLE_SIZE,\n",
    "            \"rng_state\": RNG_STATE,\n",
    "            \"outputs\": {\n",
    "                \"cleaned_csv\": str(out_path),\n",
    "                \"transform_invalid_count\": trans_invalid_count,\n",
    "                \"validator_invalid_count\": val_invalid_count\n",
    "            },\n",
    "            \"artifacts\": manifest_paths\n",
    "        }\n",
    "        man_path = write_manifest(DATA_PATH, manifest)\n",
    "        append_log(f\"[MANIFEST] {man_path}\", DATA_PATH)\n",
    "        \n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        append_log(f\"[FATAL] {traceback.format_exc()}\", DATA_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40804f",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d65be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re, textwrap, datetime as dt\n",
    "\n",
    "try:\n",
    "    from unidecode import unidecode   # Akzente → ASCII\n",
    "except ImportError:\n",
    "    unidecode = lambda s: s\n",
    "\n",
    "# Eingabe-/Ausgabe\n",
    "CLEAN_DIR = Path(\"cleaned\")\n",
    "MERGE_OUT = Path(\"merged/all_movies_wide_fuzzy.csv\")\n",
    "DUPL_OUT  = Path(\"merged/all_movies_fuzzy_duplicates.csv\")\n",
    "\n",
    "# Kanonische Rating-Spalten je Quelle\n",
    "CANON = {\n",
    "    \"imdb_data\":              \"rating_imdb\",\n",
    "    \"movielens_aggregated\":   \"rating_movielens\",\n",
    "    \"metacritic_movies\":      \"rating_metacritic\",\n",
    "    \"rotten_tomatoes_movies\": \"rating_rt_audience\",\n",
    "}\n",
    "\n",
    "# Kanonische ID-Spalten je Quelle\n",
    "ID_MAP = {\n",
    "    \"imdb_data\":              \"ID_IMDB\",\n",
    "    \"movielens_aggregated\":   \"ID_MOVIELENS\",\n",
    "    \"metacritic_movies\":      \"ID_METACRITIC\",\n",
    "    \"rotten_tomatoes_movies\": \"ID_RT\",\n",
    "}\n",
    "\n",
    "def standardize_id_column(df: pd.DataFrame, src: str) -> pd.DataFrame:\n",
    "    target = ID_MAP.get(src, f\"ID_{src.upper()}\")\n",
    "    if target in df.columns:\n",
    "        pass\n",
    "    elif \"ID\" in df.columns:\n",
    "        df = df.rename(columns={\"ID\": target})\n",
    "    else:\n",
    "        cands = [c for c in df.columns if str(c).upper().startswith(\"ID_\")]\n",
    "        if cands:\n",
    "            df = df.rename(columns={cands[0]: target})\n",
    "    if target in df.columns:\n",
    "        df[target] = pd.to_numeric(df[target], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "# 1) Clean-CSVs einsammeln\n",
    "frames = []\n",
    "for csv in CLEAN_DIR.glob(\"*.csv\"):\n",
    "    name = csv.name\n",
    "    if (\n",
    "        name.endswith((\"merged.csv\", \"all_movies_wide.csv\", \"all_movies_wide_fuzzy.csv\"))\n",
    "        or \"_invalid_records\" in name\n",
    "        or \"_duplicates\"      in name\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    src = csv.stem  # imdb_data, movielens_aggregated, …\n",
    "    df  = pd.read_csv(csv)\n",
    "\n",
    "    # Jahr vereinheitlichen\n",
    "    if \"year\" in df.columns and \"release_year\" not in df.columns:\n",
    "        df = df.rename(columns={\"year\": \"release_year\"})\n",
    "\n",
    "    # Rating in kanonische Spalte umbenennen (falls nötig)\n",
    "    rating_col = CANON.get(src, f\"rating_{src}\")\n",
    "    if \"rating\" in df.columns and rating_col not in df.columns:\n",
    "        df = df.rename(columns={\"rating\": rating_col})\n",
    "\n",
    "    # ID-Spalte standardisieren\n",
    "    df = standardize_id_column(df, src)\n",
    "\n",
    "    df[\"source\"] = src\n",
    "    frames.append(df)\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(f\"Keine geeigneten Clean-CSVs in {CLEAN_DIR}\")\n",
    "\n",
    "long_df = pd.concat(frames, ignore_index=True)\n",
    "long_df[\"norm_title\"]   = long_df[\"title\"].map(norm_title)\n",
    "long_df[\"release_year\"] = pd.to_numeric(long_df[\"release_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# 3) Jahr-Cluster (±1) – wie statisch\n",
    "def year_cluster(sub: pd.DataFrame) -> pd.Series:\n",
    "    years = sorted(set([y for y in sub[\"release_year\"].dropna()]))\n",
    "    clusters, cid = {}, 0\n",
    "    for y in years:\n",
    "        if any(abs(y - c) <= 1 for c in clusters.get(cid, [])):\n",
    "            clusters[cid].append(y)\n",
    "        else:\n",
    "            cid += 1; clusters[cid] = [y]\n",
    "    mapping = {y: c for c, ys in clusters.items() for y in ys}\n",
    "    return sub[\"release_year\"].map(mapping).fillna(cid + 1).astype(int)\n",
    "\n",
    "long_df[\"year_cluster\"] = (\n",
    "    long_df.groupby(\"norm_title\", group_keys=False)\n",
    "           .apply(year_cluster)\n",
    ")\n",
    "\n",
    "# 4) Helper für gruppierte Aggregation\n",
    "def first_valid(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if not s.empty else np.nan\n",
    "\n",
    "def first_non_empty(series: pd.Series):\n",
    "    for val in series:\n",
    "        if isinstance(val, list) and val:\n",
    "            return val\n",
    "        if isinstance(val, str) and val.strip():\n",
    "            return val\n",
    "    return np.nan\n",
    "\n",
    "group_cols = [\"norm_title\", \"year_cluster\"]\n",
    "rating_cols = [c for c in long_df.columns if c.startswith(\"rating_\")]\n",
    "id_cols     = [c for c in long_df.columns if str(c).startswith(\"ID_\")]\n",
    "\n",
    "# 5) Ratings je Film (erste gültige) – entspricht statischem Pivot-Resultat\n",
    "ratings_wide = (\n",
    "    long_df.groupby(group_cols, as_index=False)[rating_cols]\n",
    "           .agg(first_valid) if rating_cols else long_df[group_cols].drop_duplicates()\n",
    ")\n",
    "\n",
    "# IDs je Film (erste gültige)\n",
    "ids_map = (\n",
    "    long_df.groupby(group_cols, as_index=False)[id_cols]\n",
    "           .agg(first_valid) if id_cols else long_df[group_cols].drop_duplicates()\n",
    ")\n",
    "\n",
    "# 6) Repräsentativer Titel & kleinstes Jahr\n",
    "meta = (\n",
    "    long_df.sort_values([\"source\", \"title\"])\n",
    "           .groupby(group_cols, as_index=False)\n",
    "           .agg(title=(\"title\", \"first\"), release_year=(\"release_year\", \"min\"))\n",
    ")\n",
    "\n",
    "# 7) Genres: erste nicht-leere Liste (wie statisch)\n",
    "genres_map = (\n",
    "    long_df.sort_values(\"source\")\n",
    "           .groupby(group_cols)[\"genres\"]\n",
    "           .apply(first_non_empty)\n",
    "           .reset_index(name=\"genres\")\n",
    ")\n",
    "\n",
    "# 8) Wide zusammenführen\n",
    "wide = ratings_wide.merge(meta,       on=group_cols, how=\"left\")\n",
    "wide = wide.merge(genres_map,         on=group_cols, how=\"left\")\n",
    "wide = wide.merge(ids_map,            on=group_cols, how=\"left\")\n",
    "\n",
    "# 9) Ungefiltertes Ergebnis speichern\n",
    "MERGE_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "UNFILTERED_OUT = Path(\"merged/all_movies.csv\")\n",
    "wide.to_csv(UNFILTERED_OUT, index=False)\n",
    "print(f\"💾 Ungefilterter Wide-Frame gespeichert: {UNFILTERED_OUT}\")\n",
    "\n",
    "# 10) ≥2 Ratings-Filter (wie statisch)\n",
    "present_rating_mask = wide[rating_cols].notna().sum(axis=1) if rating_cols else pd.Series(0, index=wide.index)\n",
    "wide_filtered = wide.loc[present_rating_mask >= 2].copy()\n",
    "\n",
    "# 11) Finale Spalten (IDs nach vorne)\n",
    "ordered_cols = (id_cols + [\"title\", \"release_year\", \"genres\"] + rating_cols)\n",
    "ordered_cols = [c for c in ordered_cols if c in wide_filtered.columns]\n",
    "wide_filtered = wide_filtered[ordered_cols]\n",
    "\n",
    "# 12) Fuzzy-Duplikate (title, release_year)\n",
    "dup_mask   = wide_filtered.duplicated(subset=[\"title\", \"release_year\"], keep=False)\n",
    "duplicates = wide_filtered[dup_mask].copy()\n",
    "uniques    = wide_filtered[~dup_mask].copy()\n",
    "\n",
    "# 13) Endergebnisse speichern\n",
    "uniques.to_csv(MERGE_OUT, index=False)\n",
    "duplicates.to_csv(DUPL_OUT, index=False)\n",
    "\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "  — Fuzzy-Merge abgeschlossen ({dt.date.today()}) —\n",
    "  Eingelesene Quellen : {len(frames)}\n",
    "  Long-Records        : {len(long_df)}\n",
    "  Filme mit ≥2 Votes  : {len(uniques)}\n",
    "  Fuzzy-Duplikate     : {len(duplicates)}\n",
    "  👉 Gemergt  : {MERGE_OUT}\n",
    "  👉 Duplikate: {DUPL_OUT}\n",
    "\"\"\").strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6075eaf",
   "metadata": {},
   "source": [
    "Superscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Superscore (0-10 Skala, ungewichtet – wie static_pipeline) ──────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap, datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "MERGED      = Path(\"merged/all_movies_wide_fuzzy.csv\")\n",
    "OUT_CSV     = Path(\"merged/all_movies_superscore_adaptive.csv\")\n",
    "MIN_RATINGS = 2\n",
    "\n",
    "df = pd.read_csv(MERGED)\n",
    "\n",
    "# ID-Spalten erkennen und sauber typisieren (bleiben bis zum Ende erhalten)\n",
    "id_cols = [c for c in df.columns if c == \"ID\" or str(c).startswith(\"ID_\")]\n",
    "for c in id_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Rating-Spalten finden\n",
    "rating_cols = [c for c in df.columns if c.startswith(\"rating_\")]\n",
    "if not rating_cols:\n",
    "    raise RuntimeError(\"Keine rating_*-Spalten gefunden.\")\n",
    "\n",
    "# 1. Quelle → 0-10-Normierung (heuristisch wie in static_pipeline)\n",
    "def to_0_10(series):\n",
    "    if series.dropna().empty:\n",
    "        return series\n",
    "    mx = series.max()\n",
    "    if mx <= 5.5:      # 0–5 Skala → *2 (MovieLens)\n",
    "        return series * 2\n",
    "    if mx > 10:        # 0–100 Skala → /10 (Metacritic, RT%)\n",
    "        return series / 10\n",
    "    return series      # 0–10 unverändert (IMDb)\n",
    "\n",
    "norm_cols = []\n",
    "for col in rating_cols:\n",
    "    ncol = col.replace(\"rating_\", \"\") + \"_norm\"\n",
    "    df[ncol] = to_0_10(df[col])\n",
    "    norm_cols.append(ncol)\n",
    "\n",
    "# 2. Anzahl verfügbarer normalisierter Ratings\n",
    "df[\"num_available_ratings\"] = df[norm_cols].notna().sum(axis=1)\n",
    "\n",
    "# 3. Superscore (nur wenn ≥ MIN_RATINGS normalisierte Ratings vorhanden)\n",
    "mask = df[\"num_available_ratings\"] >= MIN_RATINGS\n",
    "df.loc[mask, \"superscore_mean_0_10\"]   = df.loc[mask, norm_cols].mean(axis=1).round(1)\n",
    "df.loc[mask, \"superscore_median_0_10\"] = df.loc[mask, norm_cols].median(axis=1).round(1)\n",
    "\n",
    "# 4. Ausgabe – ID-Spalten vorne\n",
    "base_cols = [c for c in [\"title\", \"release_year\", \"genres\"] if c in df.columns]\n",
    "tail_cols = [\"num_available_ratings\", \"superscore_mean_0_10\", \"superscore_median_0_10\"]\n",
    "cols_out  = id_cols + base_cols + rating_cols + norm_cols + tail_cols\n",
    "cols_out  = [c for c in cols_out if c in df.columns]\n",
    "\n",
    "df[cols_out].to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "  — Superscore-Static abgeschlossen ({dt.date.today()}) —\n",
    "  Rating-Spalten erkannt : {rating_cols}\n",
    "  Normierte Spalten      : {norm_cols}\n",
    "  Filme mit ≥{MIN_RATINGS} Ratings : {int(mask.sum())} / {len(df)}\n",
    "  ID-Spalten im Output   : {id_cols}\n",
    "  👉 Ergebnis : {OUT_CSV}\n",
    "\"\"\").strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
